---
title: 【机器学习】一文读懂分类算法常用评价指标
tags: [机器学习, 深度学习, 分类算法, 评价指标, 分类算法评价指标, 准确率、精确率、召回率, P-R曲线, F1指数, ROC, AUC]
date: 2019-09-01 13:01:14
permalink: classification-metrics
categories: Machine Learning
mathjax: true
---

## 前言

评价指标是针对将相同的数据，输入不同的算法模型，或者输入不同参数的同一种算法模型，而给出这个算法或者参数好坏的定量指标。

在模型评估过程中，往往需要使用多种不同的指标进行评估，在诸多的评价指标中，大部分指标只能片面的反应模型的一部分性能，如果不能合理的运用评估指标，不仅不能发现模型本身的问题，而且会得出错误的结论。

最近恰好在做文本分类的工作，所以把机器学习分类任务的评价指标又过了一遍。本文将详细介绍机器学习分类任务的常用评价指标：准确率（Accuracy）、精确率（Precision）、召回率（Recall）、P-R曲线（Precision-Recall Curve）、F1 Score、混淆矩阵（Confuse Matrix）、ROC、AUC。

## 准确率（Accuracy）

准确率是分类问题中最为原始的评价指标，准确率的定义是**预测正确的结果占总样本的百分比**，其公式如下：
$$
Accuracy = \frac{TP+TN}{TP+TN+FP+FN}
$$
其中：

- 真正例(True Positive, **TP**)：被模型预测为正的正样本；
- 假正例(False Positive, **FP**)：被模型预测为正的负样本；
- 假负例(False Negative, **FN**)：被模型预测为负的正样本；
- 真负例(True Negative, **TN**)：被模型预测为负的负样本；

但是，准确率评价算法有一个明显的弊端问题，就是在数据的类别不均衡，特别是有极偏的数据存在的情况下，准确率这个评价指标是不能客观评价算法的优劣的。例如下面这个例子：

在测试集里，有100个sample，99个反例，只有1个正例。如果我的模型不分青红皂白对任意一个sample都预测是反例，那么我的模型的准确率就为0.99，从数值上看是非常不错的，但事实上，这样的算法没有任何的预测能力，于是我们就应该考虑是不是评价指标出了问题，这时就需要使用其他的评价指标综合评判了。

## 精确率（Precision）、召回率（Recall）

**精准率**（Precision）又叫**查准率**，它是**针对预测结果**而言的，它的含义是**在所有被预测为正的样本中实际为正的样本的概率**，意思就是在预测为正样本的结果中，我们有多少把握可以预测正确，其公式如下：
$$
Precision = \frac{TP}{TP+FP}
$$

精准率和准确率看上去有些类似，但是完全不同的两个概念。精准率代表对正样本结果中的预测准确程度，而准确率则代表整体的预测准确程度，既包括正样本，也包括负样本。

**召回率**（Recall）又叫**查全率**，它是**针对原样本**而言的，它的含义是**在实际为正的样本中被预测为正样本的概率**，其公式如下：
$$
Recall = \frac{TP}{TP+FN}
$$

引用Wiki中的图，帮助说明下二者的关系。

![Precision And Recall](http://pic.guoyaohua.com/image/classification_metrics/Precisionrecall.png)

在不同的应用场景下，我们的关注点不同，例如，在预测股票的时候，我们更关心精准率，即我们预测升的那些股票里，真的升了有多少，因为那些我们预测升的股票都是我们投钱的。而在预测病患的场景下，我们更关注召回率，即真的患病的那些人里我们预测错了情况应该越少越好。

精确率和召回率是一对此消彼长的度量。例如在推荐系统中，我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，召回率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样准确率就很低了。

在实际工程中，我们往往需要结合两个指标的结果，去寻找一个平衡点，使综合性能最大化。

## P-R曲线

P-R曲线（Precision Recall Curve）正是描述精确率、召回率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示：

![Precision Recall Curve](http://pic.guoyaohua.com/image/classification_metrics/PR_Curve.png)

P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“**平衡点**”（Break-Event Point，简称**BEP**），即当P=R时的取值，平衡点的取值越高，性能更优。

## F1-Score

正如上文所述，Precision和Recall指标有时是此消彼长的，即精准率高了，召回率就下降，在一些场景下要兼顾精准率和召回率，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的**加权调和平均**，即：
$$
\frac{1}{F_{\beta}}=\frac{1}{1+\beta^{2}} \cdot\left(\frac{1}{P}+\frac{\beta^{2}}{R}\right)
$$


$$
F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}
$$


特别地，当β=1时，也就是常见的F1-Score，是P和R的调和平均，当F1较高时，模型的性能越好。


$$
\frac{1}{F 1}=\frac{1}{2} \cdot\left(\frac{1}{P}+\frac{1}{R}\right)
$$


$$
F1=\frac{2 \times P \times R}{P+R} = \frac{2 \times TP}{样例总数+TP-TN}
$$

## ROC曲线

ROC以及后面要讲到的AUC，是分类任务中非常常用的评价指标，本文将详细阐述。可能有人会有疑问，既然已经这么多评价标准，为什么还要使用ROC和AUC呢？

因为ROC曲线有个很好的特性：**当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。**在实际的数据集中经常会出现类别不平衡（Class Imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化，ROC以及AUC可以很好的消除样本类别不平衡对指标结果产生的影响。 

另一个原因是，ROC和上面做提到的P-R曲线一样，是一种不依赖于阈值（Threshold）的评价指标，在输出为概率分布的分类模型中，如果仅使用准确率、精确率、召回率作为评价指标进行模型对比时，都必须时基于某一个给定阈值的，对于不同的阈值，各模型的Metrics结果也会有所不同，这样就很难得出一个很置信的结果。

在正式介绍ROC之前，我们还要再介绍两个指标，**这两个指标的选择使得ROC可以无视样本的不平衡。**这两个指标分别是：**灵敏度（sensitivity）**和**特异度（specificity）**，也叫做**真正率（TPR）**和**假正率（FPR）**，具体公式如下。

- 真正率(True Positive Rate , **TPR**)，又称灵敏度：

$$
TPR = \frac{正样本预测正确数}{正样本总数} = \frac{TP}{TP+FN}
$$

其实我们可以发现灵敏度和召回率是一模一样的，只是名字换了而已

- 假负率(False Negative Rate , **FNR**) ：

$$
FNR = \frac{正样本预测错误数}{正样本总数} = \frac{FN}{TP+FN}
$$

- 假正率(False Positive Rate , **FPR**) ：

$$
FPR = \frac{负样本预测错误数}{负样本总数} = \frac{FP}{TN+FP}
$$

- 真负率(True Negative Rate , **TNR**)，又称特异度：

$$
TNR = \frac{负样本预测正确数}{负样本总数} = \frac{TN}{TN+FP}
$$

细分析上述公式，我们可以可看出，灵敏度（真正率）TPR是正样本的召回率，特异度（真负率）TNR是负样本的召回率，而假负率$FNR=1-TPR$、假正率$FPR=1-TNR$，上述四个量都是针对单一类别的预测结果而言的，所以对整体样本是否均衡并不敏感。举个例子：假设总样本中，90%是正样本，10%是负样本。在这种情况下我们如果使用准确率进行评价是不科学的，但是用TPR和TNR却是可以的，因为TPR只关注90%正样本中有多少是被预测正确的，而与那10%负样本毫无关系，同理，FPR只关注10%负样本中有多少是被预测错误的，也与那90%正样本毫无关系。这样就避免了样本不平衡的问题。

**ROC（Receiver Operating Characteristic）曲线**，又称接受者操作特征曲线。该曲线最早应用于雷达信号检测领域，用于区分信号与噪声。后来人们将其用于评价模型的预测能力。ROC曲线中的主要两个指标就是**真正率TPR**和**假正率FPR**，上面已经解释了这么选择的好处所在。其中横坐标为假正率（FPR），纵坐标为真正率（TPR），下面就是一个标准的ROC曲线图。

![ROC Curve](http://pic.guoyaohua.com/image/classification_metrics/ROC.png)

### 阈值问题

与前面的P-R曲线类似，ROC曲线也是通过**遍历所有阈值**来绘制整条曲线的。如果我们不断的遍历所有阈值，预测的正样本和负样本是在不断变化的，相应的在ROC曲线图中也会沿着曲线滑动。

![ROC-Threshold](http://pic.guoyaohua.com/image/classification_metrics/ROC-Threshold.gif)

我们看到改变阈值只是不断地改变预测的正负样本数，即TPR和FPR，但是曲线本身并没有改变。这是有道理的，阈值并不会改变模型的性能。

### 判断模型性能

那么如何判断一个模型的ROC曲线是好的呢？这个还是要回归到我们的目的：FPR表示模型对于负样本误判的程度，而TPR表示模型对正样本召回的程度。我们所希望的当然是：负样本误判的越少越好，正样本召回的越多越好。所以总结一下就是**TPR越高，同时FPR越低（即ROC曲线越陡），那么模型的性能就越好。**参考如下动态图进行理解。

![ROC Curve Change](http://pic.guoyaohua.com/image/classification_metrics/ROC-Performance.gif)

即：**进行模型的性能比较时，与PR曲线类似，若一个模型A的ROC曲线被另一个模型B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。**

### 无视样本不平衡

前面已经对ROC曲线为什么可以无视样本不平衡做了解释，下面我们用动态图的形式再次展示一下它是如何工作的。我们发现：**无论红蓝色样本比例如何改变，ROC曲线都没有影响。**

![](http://pic.guoyaohua.com/image/classification_metrics/ROC-Balance.gif)

## AUC

AUC(Area Under Curve)又称为曲线下面积，是处于ROC Curve下方的那部分面积的大小。上文中我们已经提到，对于ROC曲线下方面积越大表明模型性能越好，于是AUC就是由此产生的评价指标。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。如果模型是完美的，那么它的AUC = 1，证明所有正例排在了负例的前面，如果模型是个简单的二类随机猜测模型，那么它的AUC = 0.5，如果一个模型好于另一个，则它的曲线下方面积相对较大，对应的AUC值也会较大。

### 物理意义

AUC对所有可能的分类阈值的效果进行综合衡量。首先AUC值是一个概率值，可以理解为随机挑选一个正样本以及一个负样本，分类器判定正样本分值高于负样本分值的概率就是AUC值。简言之，AUC值越大，当前的分类算法越有可能将正样本分值高于负样本分值，即能够更好的分类。

## 混淆矩阵

混淆矩阵（[Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix)）又被称为错误矩阵，通过它可以直观地观察到算法的效果。它的每一列是样本的预测分类，每一行是样本的真实分类（反过来也可以），顾名思义，它反映了分类结果的混淆程度。混淆矩阵$i$行$j$列的原始是原本是类别$i$却被分为类别$j$的样本个数，计算完之后还可以对之进行可视化：

![Confusion Matrix Heat Map](http://pic.guoyaohua.com/image/classification_metrics/Confusion_Matrix.png)

## 多分类问题

对于多分类问题，或者在二分类问题中，我们有时候会有多组混淆矩阵，例如：多次训练或者在多个数据集上训练的结果，那么估算全局性能的方法有两种，分为宏平均（macro-average）和微平均（micro-average）。简单理解，宏平均就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，再算出$Fβ$或$F1$，而微平均则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出$Fβ$或$F1$。其它分类指标同理，均可以通过宏平均/微平均计算得出。
$$
\operatorname{macro}P=\frac{1}{n} \sum_{i=1}^{n} P_{i}
$$

$$
\operatorname{macro}R=\frac{1}{n} \sum_{i=1}^{n} R_{i}
$$

$$
\operatorname{macro} F 1=\frac{2 \times \operatorname{macro} P \times \operatorname{macro} R}{\operatorname{macro} P+\operatorname{macro}R}
$$

$$
\operatorname{micro} P=\frac{\overline{T P}}{\overline{T P}+\overline{F P}}
$$

$$
\operatorname{micro}R=\frac{\overline{T P}}{\overline{T P}+\overline{F N}}
$$

$$
\operatorname{micro} F 1=\frac{2 \times \operatorname{micro} P \times \operatorname{micro} R}{\operatorname{micro} P+\operatorname{micro}R}
$$

需要注意的是，在多分类任务场景中，如果非要用一个综合考量的metric的话，**宏平均会比微平均更好一些**，因为宏平均受稀有类别影响更大。宏平均平等对待每一个类别，所以它的值主要受到稀有类别的影响，而微平均平等考虑数据集中的每一个样本，所以它的值受到常见类别的影响比较大。