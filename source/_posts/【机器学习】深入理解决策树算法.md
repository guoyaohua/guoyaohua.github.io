---
title: 【机器学习】深入理解决策树算法
tags: [机器学习, 深度学习, 决策树, 剪枝, ID3, C4.5, CART树, 信息熵, 基尼指数, 信息增益]
date: 2019-11-11 22:38:58
permalink: decision-tree
categories: Machine Learning
mathjax: true
---

## 引言

决策树(Decision Tree)是机器学习中一种经典的分类与回归算法。本文主要讨论用于分类的决策树。决策树模型呈树形结构，在分类问题中，决策树模型可以认为是`if-then`规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。决策树学习通常包括3个步骤：**特征选择**、**决策树的生成**和**决策树的剪枝**。

## 基本原理

### 模型结构

决策树由结点(Node)和有向边(Directed Edge)组成。结点有两种类型：内部结点(Internal Node)和叶结点(Leaf Node)。内部结点表示特征或属性，叶结点表示一个类别，有向边代表了划分规则。

决策树从根结点到子结点的的有向边代表了一条路径。决策树的路径是互斥并且是完备的。

用决策树分类时，对样本的某个特征进行测试，根据测试结果将样本分配到树的子结点上。此时每个子结点对应该特征的一个取值。递归地对样本测试，直到该样本被划分叶结点。最后将样本分配为叶结点所属的类。

### 条件概率分布

决策树将特征空间划分为互不相交的单元，在每个单元定义一个类的概率分布，这就构成了一个条件概率分布。

![Decision Tree](http://pic.guoyaohua.com/image/decision-tree/decision-tree-tutorial-animated3.gif)

- 决策树的每一条路径对应于划分中的一个基本单元。
- 设某个单元$\mathbb S$内部有$N_S$个样本点，则它定义了一个条件概率分布$p(y\mid \mathbf{\vec x}), \mathbf{\vec x} \in \mathbb S $。
- 单元上的条件概率偏向哪个类，则该单元划归到该类的概率较大。即单元的类别为：$ \arg\max_y p(y  \mid \mathbf{\vec x}), \mathbf{\vec x} \in \mathbb S $
- 决策树所代表的条件概率分布由各个单元给定条件下类的条件概率分布组成。

**通俗来讲，在训练过程中，我们学习构建的决策树（规则）会将训练样本划分在不同的叶子节点中，每个叶子节点所代表的类别就是该叶子节点中数量最多的样本类别，当然在某些场景下，我们会考虑训练样本的权重，来定义叶子节点代表的类别。**

### 学习过程

决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。

决策树学习的算法通常是一个**递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类**的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集己经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一棵决策树。

以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象。我们需要对已生成的树自下而上进行**剪枝**，将树变得更简单，从而使它具有更好的泛化能力。具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更髙的结点，然后将父结点或更高的结点改为新的叶结点。

如果特征数量很多，也可以在决策树学习开始的时候，对特征进行选择，只留下对训练数据有足够分类能力的特征。

可以看出，决策树学习算法包含**特征选择**、**决策树的生成**与**决策树的剪枝**过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型。决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。

决策树学习常用的算法有**ID3、C4.5与CART**，下文结合这些算法分别叙述决策树学习的特征选择、决策树的生成和剪枝过程。

## 特征选择

特征选择在于选取对训练数据具有分类能力的特征。这样可以提髙决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。

通常特征选择的准则是**信息增益**或**信息增益比**。

### 信息增益

为了便于说明，先给出熵与条件熵的定义。

在信息论与概率统计中，熵(Entropy)是表示随机变量不确定性的度量。设`X`是一个取有限个值的离散随机变量，其概率分布为：
$$
P \left \{X=x_{i}  \right \}=p_{i},\;\;\;i=1,2\cdots ,n
$$
则随机变量`X`的熵定义为：
$$
H(X) = - \sum_{i=1}^n p_{i} \log p_{i}
$$
在上式中，若$p_{i}=0$，则定义$0\log0=0$。通常，式中的对数以2为底或以自然对数`e`为底，对应熵的单位分别称作比特(Bit)或纳特(Nat)。由定义可知，熵只依赖于`X`的分布，而与`X`的取值无关，所以也可将`X`的熵记作$H(p)$，即：
$$
H(p) = - \sum_{i=1}^n p_{i} \log p_{i}
$$
熵越大，随机变量的不确定性就越大。从定义可验证：
$$
0 \leq H(p) \leq \log n
$$
当随机变量只取两个值，例如1、0时，即`X`的分布为：
$$
P(X=1)=p,\; \; \; P(X=0)=1-p,\; \; \; 0 \leq p \leq 1
$$
熵为：
$$
H(p)=-p\log_{2}p-(1 - p)log_{2}(1-p)
$$
当$p = 0$或$p = 1$时$H(p) = 0$，随机变量完全没有不确定性。当$p=0.5$时， $H(p)=1$，熵取值最大，随机变量不确定性最大。

![二元信源熵函数](http://pic.guoyaohua.com/image/decision-tree/Entropy.png)

设有随机变量`(X, Y)`，其联合概率分布为：
$$
P(X = x_{i},Y = y_{j}) = p_{ij}\;,\; \; \;i = 1,2,\cdots,n;\;\;j= 1,2,\cdots,m
$$
条件熵$H(X\mid Y)$表示在己知随机变量`X`的条件下随机变量`Y`的不确定性。随机变量`X`给定的条件下随机变量`Y`的条件熵(Conditional Entropy) $H(X\mid Y)$，定义为`X`给定条件下`Y`的条件概率分布的熵对`X`的数学期望：
$$
H(X\mid Y)=\sum_{i=1}^{n}p_{i}H(Y\mid X=X_{i})
$$
这里，$p_{i}=P(X=x_{i})\;,\; \; \;i=1,2,\cdots,n$。

当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵(Empirical Entropy)和经验条件熵(Empirical Conditional Entropy)。此时，如果有0概率，令$0\log0 = 0$。

**信息增益(Information Gain)**表示得知特征`X`的信息而使得类`Y`的信息的不确定性减少的程度。

特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D\mid A)$之差，即：
$$
g(D,A)=H(D)-H(D\mid A)
$$
—般地，熵$H(Y)$与条件熵之差$H(Y\mid X)$称为互信息(Mutual Information)。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。

决策树学习应用信息增益准则选择特征。给定训练数据集$D$和特征$A$，经验熵$H(D)$表示对数据集$D$进行分类的不确定性。而经验条件熵$H(D\mid A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性。那么它们的差，即信息增益， 就表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。显然，对于数据集$D$而言，**信息增益依赖于特征，不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。**

根据信息增益准则的特征选择方法是：对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。

设训练数据集为$D$，$\left | D \right |$表示其样本容量，即样本个数。设有`K`个类$C_{k}$，$k=1,2,\cdots ,K$，$\left | C_{k} \right |$为属于类$C_{k}$的样本个数，$\sum_{k=1}^{K}\left |C_{k}  \right |=\left | D \right |$。设特征$A$有`n`个不同的取值$\left \{ a_{1},a_{2},\cdots ,a_{n} \right \}$，根据特征$A$的取值将$D$分为`n`个子集$D_{1},D_{2},\cdots ,D_{n}$，$\left | D_{i} \right |$为$D_{i}$的样本个数，$\sum_{i=1}^{n}\left |D_{i}  \right |=\left | D \right |$。记子集$D_{i}$中属于类$C_{k}$的样本的集合为$D_{ik}$，即$D_{ik}=D_{i}\bigcap D_{k}$，$\left | D_{ik} \right |$为$D_{ik}$的样本个数。于是信息增益的算法如下：

**信息增益的算法**

​		输入：训练数据集$D$和特征$A$；

​		输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$。

​		(1)  计算数据集$D$的经验熵$H(D)$：
$$
H(D)=-\sum_{k=1}^{K}\frac{\left | C_{k} \right |}{\left | D \right |}\log_{2}{\frac{\left | C_{k} \right |}{\left | D \right |}}
$$
​		(2)  计算特征$A$对数据集$D$的经验条件熵$H(D\mid A)$：
$$
H(D\mid A)=\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}H(D_{i})=-\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}\sum_{k=1}^{K}\frac{\left | D_{ik} \right |}{\left | D_{i} \right |}\log_{2}{\frac{\left | D_{ik} \right |}{\left | D_{i} \right |}}
$$
​		(3)  计算信息增益：
$$
g(D,A)=H(D)-H(D\mid A)
$$

### 信息增益比

信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大。反之，信息增益值会偏小。使用信息增益比(Information Gain Ratio)可以对这一问题进行校正。这是特征选择的另一准则。

特征$A$对训练数据集$D$的信息增益比$g_{R}(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$的经验上$H(D)$之比：
$$
g_{R}(D,A)=\frac{g(D,A)}{H(D)}
$$

## 生成算法

### ID3算法

ID3算法的核心是在决策树各个结点上应用**信息增益准则**选择特征，递归地构建决策树。具体方法是：从根结点(Root Node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。具体算法过程如下：

**ID3的生成算法**

​		输入：训练数据集$D$，特征集$A$，阈值$\varepsilon $；

​		输出：决策树$T$。

​		(1)  若$D$中所有实例属于同一类$C_{k}$，则$T$为单结点树，并将类作为该结点的类标记，返回$T$；

​		(2)  若$A= \varnothing $，则$T$为单结点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T$；

​		(3)  否则，计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_{g}$；

​		(4)  如果$A_{g}$的信息增益小于阈值$\varepsilon $，则置$T$为单结点树，并将$D$中实例数最大的类作为该结点的类标记，返回$T$；

​		(5)  否则，对$A_{g}$的每一可能值$a_{i}$，依$A_{g}=a_{i}$将$D$分割为若干非空子集$D_{i}$，将 $D_{i}$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$。

​		(6)  对第`i`个子结点，以$D_{i}$为训练集，以为$A-\left \{ A_{g} \right \}$特征集，递归地调用步(1)〜步(5)，得到子树$T_{i}$，返回$T_{i}$。

ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。

### C4.5算法

C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进。C4.5在生成的过程中，用**信息增益比**来选择特征。

**C4.5的生成算法**

​		输入：训练数据集$D$，特征集$A$，阈值$\varepsilon $；

​		输出：决策树$T$。

​		(1)  若$D$中所有实例属于同一类$C_{k}$，则$T$为单结点树，并将类作为该结点的类标记，返回$T$；

​		(2)  若$A= \varnothing $，则$T$为单结点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T$；

​		(3)  否则，计算$A$中各特征对$D$的信息增益比，选择信息增益比最大的特征$A_{g}$；

​		(4)  如果$A_{g}$的信息增益比小于阈值$\varepsilon $，则置$T$为单结点树，并将$D$中实例数最大的类作为该结点的类标记，返回$T$；

​		(5)  否则，对$A_{g}$的每一可能值$a_{i}$，依$A_{g}=a_{i}$将$D$分割为若干非空子集$D_{i}$，将 $D_{i}$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$。

​		(6)  对第`i`个子结点，以$D_{i}$为训练集，以为$A-\left \{ A_{g} \right \}$特征集，递归地调用步(1)〜步(5)，得到子树$T_{i}$，返回$T_{i}$。

## 剪枝算法

决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。

在决策树学习中将已生成的树进行简化的过程称为剪枝(Pruning)。具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型。

决策树的剪枝往往通过极小化决策树整体的损失函数(Loss Function)或代价函数(Cost Function)来实现。设树$T$的叶结点个数为$\left | T \right |$，`t`是树$T$的叶结点，该叶结点有$N_{t}$个样本点，其中`k`类的样本点有$N_{tk}$个，$k=1,2,\cdots ,K$，$H_{t}(T)$为叶结`t`点上的经验熵，$a\geq 0$为参数，则决策树学习的损失函数可以定义为：
$$
C_{a}(T)=\sum_{t=1}^{\left | T \right |}N_{t}H_{t}(T)+a\left | T \right |
$$
其中经验熵为：
$$
H_{t}(T)=-\sum_{k}\frac{N_{tk}}{N_{t}}\log \frac{N_{tk}}{N_{t}}
$$
在损失函数中，记：
$$
C(T)=\sum_{t=1}^{\left | T \right |}N_{t}H_{t}(T)=-\sum_{t=1}^{\left | T \right |}\sum_{k=1}^{K}N_{tk}\log \frac{N_{tk}}{N_{t}}
$$
这时有：
$$
C_{a}(T)=C(T)+a|T|
$$
上式中，$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂度，也称正则项，参数$a>0$控制两者之间的影响。较大的$a$促使选择较简单的模型（树），较小的$a$促使选择较复杂的模型（树）。$a = 0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。

剪枝，就是当$a$确定时，选择损失函数最小的模型，即损失函数最小的子树。当$a$值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越髙；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好。损失函数正好表示了对两者的平衡.

可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。

![决策树剪枝过程](http://pic.guoyaohua.com/image/decision-tree/Pruning.jpg)

**树的剪枝算法**

​		输入：生成算法产生的整个树$T$，参数$a$：

​		输出：修剪后的子树$T$。

​		(1)  计算每个结点的经验熵；

​		(2)  递归地从树的叶结点向上回缩。设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_{B}$和$T_{A}$，其对应的损失函数值分别是$C_{a}(T_{B})$与$C_{a}(T_{A})$，如果$C_{a}(T_{A}) \leqslant C_{a}(T_{B})$，则进行剪枝，即将父结点变为新的叶结点。

​		(3)  返回(2)，直至不能继续为止，得到损失函数最小的子树$T_{a}$。

## CART 树

分类与回归树(Classification And Regression Tree, CART)模型由Breiman等人在1984年提出，是应用广泛的决策树学习方法。CART同样由特征选择、树的生成及剪枝组成，既可以用于分类也可以用于回归。以下将用于分类与回归的树统称为决策树。

CART是在给定输入随机变量`X`条件下输出随机变量`Y`的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。

CART算法由以下两步组成：

​		(1)  决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大；

​		(2)  决策树剪枝：用验证数据集对己生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。

### CART 生成

决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数(Gini Index)最小化准则，进行特征选择，生成二叉树。

1. 回归树的生成

假设`X`与`Y`分别为输入和输出变量，并且`Y`是连续变量，给定训练数据集
$$
D=\left \{ (x_{1},y_{1}),(x_{2},y_{2}),\cdots ,(x_{N},y_{N}) \right \}
$$
一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为`M`个单元，$R_{1},R_{2},\cdots ,R_{M}$，并且在每个单元$R_{m}$上有一个固定的输出值$c_{m}$，于是回归树模型可表示为：

$$
f(x)=\sum_{m=1}^{M}c_{m}I(x\in R_{m})
$$
当输入空间的划分确定时，可以用平方误差$\sum_{x_{i}\in R_{m}}(y_{i}-f(x_{i}))^2$来表示回归树对于训练数据的预测误差，用平方误差最小的准求解每个单元上的最优输出值。易知，单元$R_{m}$上的$c_{m}$的最优值$\hat{c}_{m}$是$R_{m}$的所有输入实例$x_{i}$对应的输出$y_{i}$的均值，即：

$$
\hat{c}_{m}=avg(y_{i}\mid x_{i}\in R_{m})
$$
采用启发式的方法对输入空间进行划分，选择第`j`个变量$x^{(j)}$和它取的值`s`，作为切分变量(Splitting Variable)和切分点(Splitting Point)， 并定义两个区域：

$$
R_{1}(j,s)=\left \{ x\mid x^{(j)}\leq s \right \}\;\;和\;\;R_{2}(j,s)=\left \{ x\mid x^{(j)}> s \right \}
$$
然后寻找最优切分变量`j`和最优切分点`s`，具体地，求解

$$
\underset{j,s}{min}\left [ \underset{c_{1}}{min}\sum_{x_{i}\in R_{1}(j,s)}(y_{i}-c_{1})^{2} + \underset{c_{2}}{min}\sum_{x_{i}\in R_{2}(j,s)}(y_{i}-c_{2})^{2} \right ]
$$
对固定输入变量`j`可以找到最优切分点`s`。

$$
\hat{c}_{1}=avg(yi\mid x_{i}\in R_{1}(j,s))\;\;和\;\;\hat{c}_{2}=avg(yi\mid x_{i}\in R_{2}(j,s))
$$
遍历所有输入变量，找到最优的切分变量`j`，构成一个对$(j,s)$。依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树。这样的回归树通常称为最小二乘回归树(Least Squares Regression Tree)。

2. 分类树的生成

分类树用基尼指数选择最优特征，同时决定该特征的最有二值分割点。

在分类问题中，假设有`K`个类，样本点属于第`k`类的概率为$p_{k}$，则概率分布的基尼指数定义为：
$$
Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}
$$
对于二分类问题，若样本点属于第一个类的概率是`p`，则概率分部的基尼指数为：
$$
Gini(p)=2p(1-p)
$$
对于给定的样本集合$D$，其基尼指数为：
$$
Gini(p)=1-\sum_{k=1}^{K}\left ( \frac{|C_{k}|}{|D|} \right )^2
$$
这里，$C_{k}$是$D$中属于第`k`类的样本子集，`K`是类的个数。

如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_{1}$和$D_{2}$两部分，即：
$$
D_{1}=\left \{ (x,y)\in D \mid A(x)=a \right \},\;\;D_{2}=D-D_{1}
$$
则在特征$A$的条件下，集合$D$的基尼指数定义为：

$$
Gini(D,A)=\frac{|D_{1}|}{|D|}Gini(D_{1})+\frac{|D_{2}|}{|D|}Gini(D_{2})
$$
基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D,A)$表示经$A *=* a$分割后集合$D$的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。

**CART生成算法**

​		输入：训练数据集$D$，停止计算的条件；

​		输出：CART决策树。

​		根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树：

​		(1)  设结点的训练数据集为$D$，计算现有特征对该数据集的基尼指数。此时，对每一个特征对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或 “否”将$D$分割成$D_{1}$和$D_{2}$两部分，计算$A=a$时的基尼指数。

​		(2)  在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依据最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。

​		(3)  对两个子结点递归地调用上述操作，直至满足停止条件为止。

​		(4)  生成CART决策树。

**算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征。**

### CART 剪枝

CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测，提高模型泛化能力。CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_{0}$底端开始不断剪枝，直到$T_{0}$的根结点，形成一个子树序列$\left \{  T_{0},T_{1}, \cdots ,T_{n} \right \}$；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。


在剪枝过程中，计算子树的损失函数：

$$
C_{a}(T)=C(T)+a|T|
$$
其中，$T$为任意子树，$C(T)$为对训练数据的预测误差（如基尼指数），$|T|$为子树的叶结点个数，$a$为参数，$C_{a}(T)$为参数是$a$时的子树$T$的整体损失。参数$a$权衡训练数据的拟合程度与模型的复杂度。

对固定的$a$，—定存在使损失函数最$C_{a}(T)$小的子树，将其表示为$T_{a}$；在损失函数$C_{a}(T)$最小的意义下是最优的。容易验证这样的最优子树是唯一的。当$a$大的时候，最优子树$T_{a}$偏小；当$a$小的时候，最优子树$T_{a}$偏大。极端情况，当$a=0$时，整体树是最优的。当$a\rightarrow \infty $时，根结点组成的单结点树是最优的。

可以用递归的方法对树进行剪枝。将$a$从小增大，$0=a_{0}<a_{1}<\cdots <a_{n}<+\infty $，产生一系列的区间味$[a_{i},a_{i+1}),\;\;i=0,1,\cdots ,n$；剪枝得到的子树序列对应着区间$a\in [a_{i},a_{i+1}),\;\;i=0,1,\cdots ,n$的最优子树序列$\left \{  T_{0},T_{1}, \cdots ,T_{n} \right \}$，序列中的子树是嵌套的。

具体地，从整体树$T_{0}$开始剪枝，对$T_{0}$的任意内部结点`t`，以`t`为单结点树的损失函数是：

$$
C_{a}(T)=C(T)+a
$$
以`t`为根结点的子树$T_{t}$的损失函数是：

$$
C_{a}(T)=C(T)+a|T_{t}|
$$
当$a = 0$及$a$充分小时，有不等式：

$$
C_{a}(T_{t})<C_{a}(t)
$$
当增大时，在某一$a$有：

$$
C_{a}(T_{t})=C_{a}(t)
$$
当$a$再增大时，$C_{a}(T_{t})>C_{a}(t)$，只要$a=\frac{C(t)-C(T_{t})}{|T_{t}|-1}$，$T_{t}$与`t`有相同的损失函数值，而`t`的结点少，因此`t`比$T_{t}$更可取，对$T_{t}$进行剪枝。

为此，对$T_{0}$中每一内部节点`t`，计算：
$$
g(t)=\frac{C(t)-C(T_{t})}{|T_{t}|-1}
$$
它表示剪枝后整体损失函数减少的程度。在$T_{0}$中减去$g(t)$最小的$T_{t}$，将得到的子树作为$T_{1}$，同时将减小的$g(t)$设置为$a_{1}$。$T_{1}$为区间$[a_{1},a_{2})$的最优子树。

如此剪枝下去，直到得到根节点。在这一过程中，不断地增加$a$的值，产生新的区间。

在剪枝得到的子树序列$\left \{  T_{0},T_{1}, \cdots ,T_{n} \right \}$中果果交叉验证选取最优子树$T_{a}$。

具体地，利用独立的验证数据集，测试子树序列$\left \{  T_{0},T_{1}, \cdots ,T_{n} \right \}$中各棵子树的平方误差或基尼指数。平方误差或基尼指数最小的决策树被认为是最优的决策树。在子树序列中，每棵子树$\left \{  T_{0},T_{1}, \cdots ,T_{n} \right \}$都对应一个参数$\left \{  a_{0},a_{1}, \cdots ,a_{n} \right \}$。所以，当最优子树$T_{k}$选定时，对应的$a_{k}$也就确定了，即得到了最优决策树$T_{a}$。

## 参考文献

1. [《统计学习方法 第一版》 P55~P74](https://baike.baidu.com/item/统计学习方法/10430179?fr=aladdin)
2. [决策树](http://www.huaxiaozhuan.com/统计学习/chapters/4_decision_tree.html)
3. [DECISION TREES TUTORIAL](https://annalyzin.wordpress.com/2016/07/27/decision-trees-tutorial/)

