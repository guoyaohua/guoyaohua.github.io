<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[【机器学习】深入理解决策树算法]]></title>
    <url>%2Fdecision-tree%2F</url>
    <content type="text"><![CDATA[引言决策树(Decision Tree)是机器学习中一种经典的分类与回归算法。本文主要讨论用于分类的决策树。决策树模型呈树形结构，在分类问题中，决策树模型可以认为是if-then规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。其主要优点是模型具有可读性，分类速度快。决策树学习通常包括3个步骤：特征选择、决策树的生成和决策树的剪枝。 基本原理模型结构决策树由结点(Node)和有向边(Directed Edge)组成。结点有两种类型：内部结点(Internal Node)和叶结点(Leaf Node)。内部结点表示特征或属性，叶结点表示一个类别，有向边代表了划分规则。 决策树从根结点到子结点的的有向边代表了一条路径。决策树的路径是互斥并且是完备的。 用决策树分类时，对样本的某个特征进行测试，根据测试结果将样本分配到树的子结点上。此时每个子结点对应该特征的一个取值。递归地对样本测试，直到该样本被划分叶结点。最后将样本分配为叶结点所属的类。 条件概率分布决策树将特征空间划分为互不相交的单元，在每个单元定义一个类的概率分布，这就构成了一个条件概率分布。 决策树的每一条路径对应于划分中的一个基本单元。 设某个单元$\mathbb S$内部有$N_S$个样本点，则它定义了一个条件概率分布$p(y\mid \mathbf{\vec x}), \mathbf{\vec x} \in \mathbb S $。 单元上的条件概率偏向哪个类，则该单元划归到该类的概率较大。即单元的类别为：$ \arg\max_y p(y \mid \mathbf{\vec x}), \mathbf{\vec x} \in \mathbb S $ 决策树所代表的条件概率分布由各个单元给定条件下类的条件概率分布组成。 通俗来讲，在训练过程中，我们学习构建的决策树（规则）会将训练样本划分在不同的叶子节点中，每个叶子节点所代表的类别就是该叶子节点中数量最多的样本类别，当然在某些场景下，我们会考虑训练样本的权重，来定义叶子节点代表的类别。 学习过程决策树学习本质上是从训练数据集中归纳出一组分类规则。与训练数据集不相矛盾的决策树（即能对训练数据进行正确分类的决策树）可能有多个，也可能一个也没有。我们需要的是一个与训练数据矛盾较小的决策树，同时具有很好的泛化能力。 决策树学习的算法通常是一个递归地选择最优特征，并根据该特征对训练数据进行分割，使得对各个子数据集有一个最好的分类的过程。这一过程对应着对特征空间的划分，也对应着决策树的构建。开始，构建根结点，将所有训练数据都放在根结点。选择一个最优特征，按照这一特征将训练数据集分割成子集，使得各个子集有一个在当前条件下最好的分类。如果这些子集己经能够被基本正确分类，那么构建叶结点，并将这些子集分到所对应的叶结点中去；如果还有子集不能被基本正确分类，那么就对这些子集选择新的最优特征，继续对其进行分割，构建相应的结点。如此递归地进行下去，直至所有训练数据子集被基本正确分类，或者没有合适的特征为止。最后每个子集都被分到叶结点上，即都有了明确的类。这就生成了一棵决策树。 以上方法生成的决策树可能对训练数据有很好的分类能力，但对未知的测试数据却未必有很好的分类能力，即可能发生过拟合现象。我们需要对已生成的树自下而上进行剪枝，将树变得更简单，从而使它具有更好的泛化能力。具体地，就是去掉过于细分的叶结点，使其回退到父结点，甚至更髙的结点，然后将父结点或更高的结点改为新的叶结点。 如果特征数量很多，也可以在决策树学习开始的时候，对特征进行选择，只留下对训练数据有足够分类能力的特征。 可以看出，决策树学习算法包含特征选择、决策树的生成与决策树的剪枝过程。由于决策树表示一个条件概率分布，所以深浅不同的决策树对应着不同复杂度的概率模型。决策树的生成对应于模型的局部选择，决策树的剪枝对应于模型的全局选择。决策树的生成只考虑局部最优，相对地，决策树的剪枝则考虑全局最优。 决策树学习常用的算法有ID3、C4.5与CART，下文结合这些算法分别叙述决策树学习的特征选择、决策树的生成和剪枝过程。 特征选择特征选择在于选取对训练数据具有分类能力的特征。这样可以提髙决策树学习的效率。如果利用一个特征进行分类的结果与随机分类的结果没有很大差别，则称这个特征是没有分类能力的。经验上扔掉这样的特征对决策树学习的精度影响不大。 通常特征选择的准则是信息增益或信息增益比。 信息增益为了便于说明，先给出熵与条件熵的定义。 在信息论与概率统计中，熵(Entropy)是表示随机变量不确定性的度量。设X是一个取有限个值的离散随机变量，其概率分布为： P \left \{X=x_{i} \right \}=p_{i},\;\;\;i=1,2\cdots ,n则随机变量X的熵定义为： H(X) = - \sum_{i=1}^n p_{i} \log p_{i}在上式中，若$p_{i}=0$，则定义$0\log0=0$。通常，式中的对数以2为底或以自然对数e为底，对应熵的单位分别称作比特(Bit)或纳特(Nat)。由定义可知，熵只依赖于X的分布，而与X的取值无关，所以也可将X的熵记作$H(p)$，即： H(p) = - \sum_{i=1}^n p_{i} \log p_{i}熵越大，随机变量的不确定性就越大。从定义可验证： 0 \leq H(p) \leq \log n当随机变量只取两个值，例如1、0时，即X的分布为： P(X=1)=p,\; \; \; P(X=0)=1-p,\; \; \; 0 \leq p \leq 1熵为： H(p)=-p\log_{2}p-(1 - p)log_{2}(1-p)当$p = 0$或$p = 1$时$H(p) = 0$，随机变量完全没有不确定性。当$p=0.5$时， $H(p)=1$，熵取值最大，随机变量不确定性最大。 设有随机变量(X, Y)，其联合概率分布为： P(X = x_{i},Y = y_{j}) = p_{ij}\;,\; \; \;i = 1,2,\cdots,n;\;\;j= 1,2,\cdots,m条件熵$H(X\mid Y)$表示在己知随机变量X的条件下随机变量Y的不确定性。随机变量X给定的条件下随机变量Y的条件熵(Conditional Entropy) $H(X\mid Y)$，定义为X给定条件下Y的条件概率分布的熵对X的数学期望： H(X\mid Y)=\sum_{i=1}^{n}p_{i}H(Y\mid X=X_{i})这里，$p_{i}=P(X=x_{i})\;,\; \; \;i=1,2,\cdots,n$。 当熵和条件熵中的概率由数据估计（特别是极大似然估计）得到时，所对应的熵与条件熵分别称为经验熵(Empirical Entropy)和经验条件熵(Empirical Conditional Entropy)。此时，如果有0概率，令$0\log0 = 0$。 信息增益(Information Gain)表示得知特征X的信息而使得类Y的信息的不确定性减少的程度。 特征$A$对训练数据集$D$的信息增益$g(D,A)$，定义为集合$D$的经验熵$H(D)$与特征$A$给定条件下$D$的经验条件熵$H(D\mid A)$之差，即： g(D,A)=H(D)-H(D\mid A)—般地，熵$H(Y)$与条件熵之差$H(Y\mid X)$称为互信息(Mutual Information)。决策树学习中的信息增益等价于训练数据集中类与特征的互信息。 决策树学习应用信息增益准则选择特征。给定训练数据集$D$和特征$A$，经验熵$H(D)$表示对数据集$D$进行分类的不确定性。而经验条件熵$H(D\mid A)$表示在特征$A$给定的条件下对数据集$D$进行分类的不确定性。那么它们的差，即信息增益， 就表示由于特征$A$而使得对数据集$D$的分类的不确定性减少的程度。显然，对于数据集$D$而言，信息增益依赖于特征，不同的特征往往具有不同的信息增益，信息增益大的特征具有更强的分类能力。 根据信息增益准则的特征选择方法是：对训练数据集（或子集）$D$，计算其每个特征的信息增益，并比较它们的大小，选择信息增益最大的特征。 设训练数据集为$D$，$\left | D \right |$表示其样本容量，即样本个数。设有K个类$C_{k}$，$k=1,2,\cdots ,K$，$\left | C_{k} \right |$为属于类$C_{k}$的样本个数，$\sum_{k=1}^{K}\left |C_{k} \right |=\left | D \right |$。设特征$A$有n个不同的取值$\left \{ a_{1},a_{2},\cdots ,a_{n} \right \}$，根据特征$A$的取值将$D$分为n个子集$D_{1},D_{2},\cdots ,D_{n}$，$\left | D_{i} \right |$为$D_{i}$的样本个数，$\sum_{i=1}^{n}\left |D_{i} \right |=\left | D \right |$。记子集$D_{i}$中属于类$C_{k}$的样本的集合为$D_{ik}$，即$D_{ik}=D_{i}\bigcap D_{k}$，$\left | D_{ik} \right |$为$D_{ik}$的样本个数。于是信息增益的算法如下： 信息增益的算法 ​ 输入：训练数据集$D$和特征$A$； ​ 输出：特征$A$对训练数据集$D$的信息增益$g(D,A)$。 ​ (1) 计算数据集$D$的经验熵$H(D)$： H(D)=-\sum_{k=1}^{K}\frac{\left | C_{k} \right |}{\left | D \right |}\log_{2}{\frac{\left | C_{k} \right |}{\left | D \right |}}​ (2) 计算特征$A$对数据集$D$的经验条件熵$H(D\mid A)$： H(D\mid A)=\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}H(D_{i})=-\sum_{i=1}^{n}\frac{\left | D_{i} \right |}{\left | D \right |}\sum_{k=1}^{K}\frac{\left | D_{ik} \right |}{\left | D_{i} \right |}\log_{2}{\frac{\left | D_{ik} \right |}{\left | D_{i} \right |}}​ (3) 计算信息增益： g(D,A)=H(D)-H(D\mid A)信息增益比信息增益值的大小是相对于训练数据集而言的，并没有绝对意义。在分类问题困难时，也就是说在训练数据集的经验熵大的时候，信息增益值会偏大。反之，信息增益值会偏小。使用信息增益比(Information Gain Ratio)可以对这一问题进行校正。这是特征选择的另一准则。 特征$A$对训练数据集$D$的信息增益比$g_{R}(D,A)$定义为其信息增益$g(D,A)$与训练数据集$D$的经验上$H(D)$之比： g_{R}(D,A)=\frac{g(D,A)}{H(D)}生成算法ID3算法ID3算法的核心是在决策树各个结点上应用信息增益准则选择特征，递归地构建决策树。具体方法是：从根结点(Root Node)开始，对结点计算所有可能的特征的信息增益，选择信息增益最大的特征作为结点的特征，由该特征的不同取值建立子结点；再对子结点递归地调用以上方法，构建决策树；直到所有特征的信息增益均很小或没有特征可以选择为止。最后得到一个决策树。ID3相当于用极大似然法进行概率模型的选择。具体算法过程如下： ID3的生成算法 ​ 输入：训练数据集$D$，特征集$A$，阈值$\varepsilon $； ​ 输出：决策树$T$。 ​ (1) 若$D$中所有实例属于同一类$C_{k}$，则$T$为单结点树，并将类作为该结点的类标记，返回$T$； ​ (2) 若$A= \varnothing $，则$T$为单结点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T$； ​ (3) 否则，计算$A$中各特征对$D$的信息增益，选择信息增益最大的特征$A_{g}$； ​ (4) 如果$A_{g}$的信息增益小于阈值$\varepsilon $，则置$T$为单结点树，并将$D$中实例数最大的类作为该结点的类标记，返回$T$； ​ (5) 否则，对$A_{g}$的每一可能值$a_{i}$，依$A_{g}=a_{i}$将$D$分割为若干非空子集$D_{i}$，将 $D_{i}$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$。 ​ (6) 对第i个子结点，以$D_{i}$为训练集，以为$A-\left \{ A_{g} \right \}$特征集，递归地调用步(1)〜步(5)，得到子树$T_{i}$，返回$T_{i}$。 ID3算法只有树的生成，所以该算法生成的树容易产生过拟合。 C4.5算法C4.5算法与ID3算法相似，C4.5算法对ID3算法进行了改进。C4.5在生成的过程中，用信息增益比来选择特征。 C4.5的生成算法 ​ 输入：训练数据集$D$，特征集$A$，阈值$\varepsilon $； ​ 输出：决策树$T$。 ​ (1) 若$D$中所有实例属于同一类$C_{k}$，则$T$为单结点树，并将类作为该结点的类标记，返回$T$； ​ (2) 若$A= \varnothing $，则$T$为单结点树，并将$D$中实例数最大的类$C_{k}$作为该结点的类标记，返回$T$； ​ (3) 否则，计算$A$中各特征对$D$的信息增益比，选择信息增益比最大的特征$A_{g}$； ​ (4) 如果$A_{g}$的信息增益比小于阈值$\varepsilon $，则置$T$为单结点树，并将$D$中实例数最大的类作为该结点的类标记，返回$T$； ​ (5) 否则，对$A_{g}$的每一可能值$a_{i}$，依$A_{g}=a_{i}$将$D$分割为若干非空子集$D_{i}$，将 $D_{i}$中实例数最大的类作为标记，构建子结点，由结点及其子结点构成树$T$，返回$T$。 ​ (6) 对第i个子结点，以$D_{i}$为训练集，以为$A-\left \{ A_{g} \right \}$特征集，递归地调用步(1)〜步(5)，得到子树$T_{i}$，返回$T_{i}$。 剪枝算法决策树生成算法递归地产生决策树，直到不能继续下去为止。这样产生的树往往对训练数据的分类很准确，但对未知的测试数据的分类却没有那么准确，即出现过拟合现象。过拟合的原因在于学习时过多地考虑如何提高对训练数据的正确分类，从而构建出过于复杂的决策树。解决这个问题的办法是考虑决策树的复杂度，对已生成的决策树进行简化。 在决策树学习中将已生成的树进行简化的过程称为剪枝(Pruning)。具体地，剪枝从已生成的树上裁掉一些子树或叶结点，并将其根结点或父结点作为新的叶结点，从而简化分类树模型。 决策树的剪枝往往通过极小化决策树整体的损失函数(Loss Function)或代价函数(Cost Function)来实现。设树$T$的叶结点个数为$\left | T \right |$，t是树$T$的叶结点，该叶结点有$N_{t}$个样本点，其中k类的样本点有$N_{tk}$个，$k=1,2,\cdots ,K$，$H_{t}(T)$为叶结t点上的经验熵，$a\geq 0$为参数，则决策树学习的损失函数可以定义为： C_{a}(T)=\sum_{t=1}^{\left | T \right |}N_{t}H_{t}(T)+a\left | T \right |其中经验熵为： H_{t}(T)=-\sum_{k}\frac{N_{tk}}{N_{t}}\log \frac{N_{tk}}{N_{t}}在损失函数中，记： C(T)=\sum_{t=1}^{\left | T \right |}N_{t}H_{t}(T)=-\sum_{t=1}^{\left | T \right |}\sum_{k=1}^{K}N_{tk}\log \frac{N_{tk}}{N_{t}}这时有： C_{a}(T)=C(T)+a|T|上式中，$C(T)$表示模型对训练数据的预测误差，即模型与训练数据的拟合程度，$|T|$表示模型复杂度，也称正则项，参数$a&gt;0$控制两者之间的影响。较大的$a$促使选择较简单的模型（树），较小的$a$促使选择较复杂的模型（树）。$a = 0$意味着只考虑模型与训练数据的拟合程度，不考虑模型的复杂度。 剪枝，就是当$a$确定时，选择损失函数最小的模型，即损失函数最小的子树。当$a$值确定时，子树越大，往往与训练数据的拟合越好，但是模型的复杂度就越髙；相反，子树越小，模型的复杂度就越低，但是往往与训练数据的拟合不好。损失函数正好表示了对两者的平衡. 可以看出，决策树生成只考虑了通过提高信息增益（或信息增益比）对训练数据进行更好的拟合。而决策树剪枝通过优化损失函数还考虑了减小模型复杂度。决策树生成学习局部的模型，而决策树剪枝学习整体的模型。 树的剪枝算法 ​ 输入：生成算法产生的整个树$T$，参数$a$： ​ 输出：修剪后的子树$T$。 ​ (1) 计算每个结点的经验熵； ​ (2) 递归地从树的叶结点向上回缩。设一组叶结点回缩到其父结点之前与之后的整体树分别为$T_{B}$和$T_{A}$，其对应的损失函数值分别是$C_{a}(T_{B})$与$C_{a}(T_{A})$，如果$C_{a}(T_{A}) \leqslant C_{a}(T_{B})$，则进行剪枝，即将父结点变为新的叶结点。 ​ (3) 返回(2)，直至不能继续为止，得到损失函数最小的子树$T_{a}$。 CART 树分类与回归树(Classification And Regression Tree, CART)模型由Breiman等人在1984年提出，是应用广泛的决策树学习方法。CART同样由特征选择、树的生成及剪枝组成，既可以用于分类也可以用于回归。以下将用于分类与回归的树统称为决策树。 CART是在给定输入随机变量X条件下输出随机变量Y的条件概率分布的学习方法。CART假设决策树是二叉树，内部结点特征的取值为“是”和“否”，左分支是取值为“是”的分支，右分支是取值为“否”的分支。这样的决策树等价于递归地二分每个特征，将输入空间即特征空间划分为有限个单元，并在这些单元上确定预测的概率分布，也就是在输入给定的条件下输出的条件概率分布。 CART算法由以下两步组成： ​ (1) 决策树生成：基于训练数据集生成决策树，生成的决策树要尽量大； ​ (2) 决策树剪枝：用验证数据集对己生成的树进行剪枝并选择最优子树，这时用损失函数最小作为剪枝的标准。 CART 生成决策树的生成就是递归地构建二叉决策树的过程。对回归树用平方误差最小化准则，对分类树用基尼指数(Gini Index)最小化准则，进行特征选择，生成二叉树。 回归树的生成 假设X与Y分别为输入和输出变量，并且Y是连续变量，给定训练数据集 D=\left \{ (x_{1},y_{1}),(x_{2},y_{2}),\cdots ,(x_{N},y_{N}) \right \}一个回归树对应着输入空间（即特征空间）的一个划分以及在划分的单元上的输出值。假设已将输入空间划分为M个单元，$R_{1},R_{2},\cdots ,R_{M}$，并且在每个单元$R_{m}$上有一个固定的输出值$c_{m}$，于是回归树模型可表示为： f(x)=\sum_{m=1}^{M}c_{m}I(x\in R_{m})当输入空间的划分确定时，可以用平方误差$\sum_{x_{i}\in R_{m}}(y_{i}-f(x_{i}))^2$来表示回归树对于训练数据的预测误差，用平方误差最小的准求解每个单元上的最优输出值。易知，单元$R_{m}$上的$c_{m}$的最优值$\hat{c}_{m}$是$R_{m}$的所有输入实例$x_{i}$对应的输出$y_{i}$的均值，即： \hat{c}_{m}=avg(y_{i}\mid x_{i}\in R_{m})采用启发式的方法对输入空间进行划分，选择第j个变量$x^{(j)}$和它取的值s，作为切分变量(Splitting Variable)和切分点(Splitting Point)， 并定义两个区域： R_{1}(j,s)=\left \{ x\mid x^{(j)}\leq s \right \}\;\;和\;\;R_{2}(j,s)=\left \{ x\mid x^{(j)}> s \right \}然后寻找最优切分变量j和最优切分点s，具体地，求解 \underset{j,s}{min}\left [ \underset{c_{1}}{min}\sum_{x_{i}\in R_{1}(j,s)}(y_{i}-c_{1})^{2} + \underset{c_{2}}{min}\sum_{x_{i}\in R_{2}(j,s)}(y_{i}-c_{2})^{2} \right ]对固定输入变量j可以找到最优切分点s。 \hat{c}_{1}=avg(yi\mid x_{i}\in R_{1}(j,s))\;\;和\;\;\hat{c}_{2}=avg(yi\mid x_{i}\in R_{2}(j,s))遍历所有输入变量，找到最优的切分变量j，构成一个对$(j,s)$。依此将输入空间划分为两个区域。接着，对每个区域重复上述划分过程，直到满足停止条件为止。这样就生成一棵回归树。这样的回归树通常称为最小二乘回归树(Least Squares Regression Tree)。 分类树的生成 分类树用基尼指数选择最优特征，同时决定该特征的最有二值分割点。 在分类问题中，假设有K个类，样本点属于第k类的概率为$p_{k}$，则概率分布的基尼指数定义为： Gini(p)=\sum_{k=1}^{K}p_{k}(1-p_{k})=1-\sum_{k=1}^{K}p_{k}^{2}对于二分类问题，若样本点属于第一个类的概率是p，则概率分部的基尼指数为： Gini(p)=2p(1-p)对于给定的样本集合$D$，其基尼指数为： Gini(p)=1-\sum_{k=1}^{K}\left ( \frac{|C_{k}|}{|D|} \right )^2这里，$C_{k}$是$D$中属于第k类的样本子集，K是类的个数。 如果样本集合$D$根据特征$A$是否取某一可能值$a$被分割成$D_{1}$和$D_{2}$两部分，即： D_{1}=\left \{ (x,y)\in D \mid A(x)=a \right \},\;\;D_{2}=D-D_{1}则在特征$A$的条件下，集合$D$的基尼指数定义为： Gini(D,A)=\frac{|D_{1}|}{|D|}Gini(D_{1})+\frac{|D_{2}|}{|D|}Gini(D_{2})基尼指数$Gini(D)$表示集合$D$的不确定性，基尼指数$Gini(D,A)$表示经$A = a$分割后集合$D$的不确定性。基尼指数值越大，样本集合的不确定性也就越大，这一点与熵相似。 CART生成算法 ​ 输入：训练数据集$D$，停止计算的条件； ​ 输出：CART决策树。 ​ 根据训练数据集，从根结点开始，递归地对每个结点进行以下操作，构建二叉决策树： ​ (1) 设结点的训练数据集为$D$，计算现有特征对该数据集的基尼指数。此时，对每一个特征对其可能取的每个值$a$，根据样本点对$A=a$的测试为“是”或 “否”将$D$分割成$D_{1}$和$D_{2}$两部分，计算$A=a$时的基尼指数。 ​ (2) 在所有可能的特征$A$以及它们所有可能的切分点$a$中，选择基尼指数最小的特征及其对应的切分点作为最优特征与最优切分点。依据最优特征与最优切分点，从现结点生成两个子结点，将训练数据集依特征分配到两个子结点中去。 ​ (3) 对两个子结点递归地调用上述操作，直至满足停止条件为止。 ​ (4) 生成CART决策树。 算法停止计算的条件是结点中的样本个数小于预定阈值，或样本集的基尼指数小于预定阈值（样本基本属于同一类），或者没有更多特征。 CART 剪枝CART剪枝算法从“完全生长”的决策树的底端剪去一些子树，使决策树变小（模型变简单），从而能够对未知数据有更准确的预测，提高模型泛化能力。CART剪枝算法由两步组成：首先从生成算法产生的决策树$T_{0}$底端开始不断剪枝，直到$T_{0}$的根结点，形成一个子树序列$\left \{ T_{0},T_{1}, \cdots ,T_{n} \right \}$；然后通过交叉验证法在独立的验证数据集上对子树序列进行测试，从中选择最优子树。 在剪枝过程中，计算子树的损失函数： C_{a}(T)=C(T)+a|T|其中，$T$为任意子树，$C(T)$为对训练数据的预测误差（如基尼指数），$|T|$为子树的叶结点个数，$a$为参数，$C_{a}(T)$为参数是$a$时的子树$T$的整体损失。参数$a$权衡训练数据的拟合程度与模型的复杂度。 对固定的$a$，—定存在使损失函数最$C_{a}(T)$小的子树，将其表示为$T_{a}$；在损失函数$C_{a}(T)$最小的意义下是最优的。容易验证这样的最优子树是唯一的。当$a$大的时候，最优子树$T_{a}$偏小；当$a$小的时候，最优子树$T_{a}$偏大。极端情况，当$a=0$时，整体树是最优的。当$a\rightarrow \infty $时，根结点组成的单结点树是最优的。 可以用递归的方法对树进行剪枝。将$a$从小增大，$0=a_{0}&lt;a_{1}&lt;\cdots &lt;a_{n}&lt;+\infty $，产生一系列的区间味$[a_{i},a_{i+1}),\;\;i=0,1,\cdots ,n$；剪枝得到的子树序列对应着区间$a\in [a_{i},a_{i+1}),\;\;i=0,1,\cdots ,n$的最优子树序列$\left \{ T_{0},T_{1}, \cdots ,T_{n} \right \}$，序列中的子树是嵌套的。 具体地，从整体树$T_{0}$开始剪枝，对$T_{0}$的任意内部结点t，以t为单结点树的损失函数是： C_{a}(T)=C(T)+a以t为根结点的子树$T_{t}$的损失函数是： C_{a}(T)=C(T)+a|T_{t}|当$a = 0$及$a$充分小时，有不等式： C_{a}(T_{t})]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>决策树</tag>
        <tag>剪枝</tag>
        <tag>ID3</tag>
        <tag>C4.5</tag>
        <tag>CART树</tag>
        <tag>信息熵</tag>
        <tag>基尼指数</tag>
        <tag>信息增益</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[【机器学习】一文读懂分类算法常用评价指标]]></title>
    <url>%2Fclassification-metrics%2F</url>
    <content type="text"><![CDATA[前言评价指标是针对将相同的数据，输入不同的算法模型，或者输入不同参数的同一种算法模型，而给出这个算法或者参数好坏的定量指标。 在模型评估过程中，往往需要使用多种不同的指标进行评估，在诸多的评价指标中，大部分指标只能片面的反应模型的一部分性能，如果不能合理的运用评估指标，不仅不能发现模型本身的问题，而且会得出错误的结论。 最近恰好在做文本分类的工作，所以把机器学习分类任务的评价指标又过了一遍。本文将详细介绍机器学习分类任务的常用评价指标：准确率（Accuracy）、精确率（Precision）、召回率（Recall）、P-R曲线（Precision-Recall Curve）、F1 Score、混淆矩阵（Confuse Matrix）、ROC、AUC。 准确率（Accuracy）准确率是分类问题中最为原始的评价指标，准确率的定义是预测正确的结果占总样本的百分比，其公式如下： Accuracy = \frac{TP+TN}{TP+TN+FP+FN}其中： 真正例(True Positive, TP)：被模型预测为正的正样本； 假正例(False Positive, FP)：被模型预测为正的负样本； 假负例(False Negative, FN)：被模型预测为负的正样本； 真负例(True Negative, TN)：被模型预测为负的负样本； 但是，准确率评价算法有一个明显的弊端问题，就是在数据的类别不均衡，特别是有极偏的数据存在的情况下，准确率这个评价指标是不能客观评价算法的优劣的。例如下面这个例子： 在测试集里，有100个sample，99个反例，只有1个正例。如果我的模型不分青红皂白对任意一个sample都预测是反例，那么我的模型的准确率就为0.99，从数值上看是非常不错的，但事实上，这样的算法没有任何的预测能力，于是我们就应该考虑是不是评价指标出了问题，这时就需要使用其他的评价指标综合评判了。 精确率（Precision）、召回率（Recall）精准率（Precision）又叫查准率，它是针对预测结果而言的，它的含义是在所有被预测为正的样本中实际为正的样本的概率，意思就是在预测为正样本的结果中，我们有多少把握可以预测正确，其公式如下： Precision = \frac{TP}{TP+FP}精准率和准确率看上去有些类似，但是完全不同的两个概念。精准率代表对正样本结果中的预测准确程度，而准确率则代表整体的预测准确程度，既包括正样本，也包括负样本。 召回率（Recall）又叫查全率，它是针对原样本而言的，它的含义是在实际为正的样本中被预测为正样本的概率，其公式如下： Recall = \frac{TP}{TP+FN}引用Wiki中的图，帮助说明下二者的关系。 在不同的应用场景下，我们的关注点不同，例如，在预测股票的时候，我们更关心精准率，即我们预测升的那些股票里，真的升了有多少，因为那些我们预测升的股票都是我们投钱的。而在预测病患的场景下，我们更关注召回率，即真的患病的那些人里我们预测错了情况应该越少越好。 精确率和召回率是一对此消彼长的度量。例如在推荐系统中，我们想让推送的内容尽可能用户全都感兴趣，那只能推送我们把握高的内容，这样就漏掉了一些用户感兴趣的内容，召回率就低了；如果想让用户感兴趣的内容都被推送，那只有将所有内容都推送上，宁可错杀一千，不可放过一个，这样准确率就很低了。 在实际工程中，我们往往需要结合两个指标的结果，去寻找一个平衡点，使综合性能最大化。 P-R曲线P-R曲线（Precision Recall Curve）正是描述精确率、召回率变化的曲线，P-R曲线定义如下：根据学习器的预测结果（一般为一个实值或概率）对测试样本进行排序，将最可能是“正例”的样本排在前面，最不可能是“正例”的排在后面，按此顺序逐个把样本作为“正例”进行预测，每次计算出当前的P值和R值，如下图所示： P-R曲线如何评估呢？若一个学习器A的P-R曲线被另一个学习器B的P-R曲线完全包住，则称：B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。但一般来说，曲线下的面积是很难进行估算的，所以衍生出了“平衡点”（Break-Event Point，简称BEP），即当P=R时的取值，平衡点的取值越高，性能更优。 F1-Score正如上文所述，Precision和Recall指标有时是此消彼长的，即精准率高了，召回率就下降，在一些场景下要兼顾精准率和召回率，最常见的方法就是F-Measure，又称F-Score。F-Measure是P和R的加权调和平均，即： \frac{1}{F_{\beta}}=\frac{1}{1+\beta^{2}} \cdot\left(\frac{1}{P}+\frac{\beta^{2}}{R}\right) F_{\beta}=\frac{\left(1+\beta^{2}\right) \times P \times R}{\left(\beta^{2} \times P\right)+R}特别地，当β=1时，也就是常见的F1-Score，是P和R的调和平均，当F1较高时，模型的性能越好。 \frac{1}{F 1}=\frac{1}{2} \cdot\left(\frac{1}{P}+\frac{1}{R}\right) F1=\frac{2 \times P \times R}{P+R} = \frac{2 \times TP}{样例总数+TP-TN}ROC曲线ROC以及后面要讲到的AUC，是分类任务中非常常用的评价指标，本文将详细阐述。可能有人会有疑问，既然已经这么多评价标准，为什么还要使用ROC和AUC呢？ 因为ROC曲线有个很好的特性：当测试集中的正负样本的分布变化的时候，ROC曲线能够保持不变。在实际的数据集中经常会出现类别不平衡（Class Imbalance）现象，即负样本比正样本多很多（或者相反），而且测试数据中的正负样本的分布也可能随着时间变化，ROC以及AUC可以很好的消除样本类别不平衡对指标结果产生的影响。 另一个原因是，ROC和上面做提到的P-R曲线一样，是一种不依赖于阈值（Threshold）的评价指标，在输出为概率分布的分类模型中，如果仅使用准确率、精确率、召回率作为评价指标进行模型对比时，都必须时基于某一个给定阈值的，对于不同的阈值，各模型的Metrics结果也会有所不同，这样就很难得出一个很置信的结果。 在正式介绍ROC之前，我们还要再介绍两个指标，这两个指标的选择使得ROC可以无视样本的不平衡。这两个指标分别是：灵敏度（sensitivity）和特异度（specificity），也叫做真正率（TPR）和假正率（FPR），具体公式如下。 真正率(True Positive Rate , TPR)，又称灵敏度： TPR = \frac{正样本预测正确数}{正样本总数} = \frac{TP}{TP+FN}其实我们可以发现灵敏度和召回率是一模一样的，只是名字换了而已 假负率(False Negative Rate , FNR) ： FNR = \frac{正样本预测错误数}{正样本总数} = \frac{FN}{TP+FN} 假正率(False Positive Rate , FPR) ： FPR = \frac{负样本预测错误数}{负样本总数} = \frac{FP}{TN+FP} 真负率(True Negative Rate , TNR)，又称特异度： TNR = \frac{负样本预测正确数}{负样本总数} = \frac{TN}{TN+FP}细分析上述公式，我们可以可看出，灵敏度（真正率）TPR是正样本的召回率，特异度（真负率）TNR是负样本的召回率，而假负率$FNR=1-TPR$、假正率$FPR=1-TNR$，上述四个量都是针对单一类别的预测结果而言的，所以对整体样本是否均衡并不敏感。举个例子：假设总样本中，90%是正样本，10%是负样本。在这种情况下我们如果使用准确率进行评价是不科学的，但是用TPR和TNR却是可以的，因为TPR只关注90%正样本中有多少是被预测正确的，而与那10%负样本毫无关系，同理，FPR只关注10%负样本中有多少是被预测错误的，也与那90%正样本毫无关系。这样就避免了样本不平衡的问题。 ROC（Receiver Operating Characteristic）曲线，又称接受者操作特征曲线。该曲线最早应用于雷达信号检测领域，用于区分信号与噪声。后来人们将其用于评价模型的预测能力。ROC曲线中的主要两个指标就是真正率TPR和假正率FPR，上面已经解释了这么选择的好处所在。其中横坐标为假正率（FPR），纵坐标为真正率（TPR），下面就是一个标准的ROC曲线图。 阈值问题与前面的P-R曲线类似，ROC曲线也是通过遍历所有阈值来绘制整条曲线的。如果我们不断的遍历所有阈值，预测的正样本和负样本是在不断变化的，相应的在ROC曲线图中也会沿着曲线滑动。 我们看到改变阈值只是不断地改变预测的正负样本数，即TPR和FPR，但是曲线本身并没有改变。这是有道理的，阈值并不会改变模型的性能。 判断模型性能那么如何判断一个模型的ROC曲线是好的呢？这个还是要回归到我们的目的：FPR表示模型对于负样本误判的程度，而TPR表示模型对正样本召回的程度。我们所希望的当然是：负样本误判的越少越好，正样本召回的越多越好。所以总结一下就是TPR越高，同时FPR越低（即ROC曲线越陡），那么模型的性能就越好。参考如下动态图进行理解。 即：进行模型的性能比较时，与PR曲线类似，若一个模型A的ROC曲线被另一个模型B的ROC曲线完全包住，则称B的性能优于A。若A和B的曲线发生了交叉，则谁的曲线下的面积大，谁的性能更优。 无视样本不平衡前面已经对ROC曲线为什么可以无视样本不平衡做了解释，下面我们用动态图的形式再次展示一下它是如何工作的。我们发现：无论红蓝色样本比例如何改变，ROC曲线都没有影响。 AUCAUC(Area Under Curve)又称为曲线下面积，是处于ROC Curve下方的那部分面积的大小。上文中我们已经提到，对于ROC曲线下方面积越大表明模型性能越好，于是AUC就是由此产生的评价指标。通常，AUC的值介于0.5到1.0之间，较大的AUC代表了较好的Performance。如果模型是完美的，那么它的AUC = 1，证明所有正例排在了负例的前面，如果模型是个简单的二类随机猜测模型，那么它的AUC = 0.5，如果一个模型好于另一个，则它的曲线下方面积相对较大，对应的AUC值也会较大。 物理意义AUC对所有可能的分类阈值的效果进行综合衡量。首先AUC值是一个概率值，可以理解为随机挑选一个正样本以及一个负样本，分类器判定正样本分值高于负样本分值的概率就是AUC值。简言之，AUC值越大，当前的分类算法越有可能将正样本分值高于负样本分值，即能够更好的分类。 混淆矩阵混淆矩阵（Confusion Matrix）又被称为错误矩阵，通过它可以直观地观察到算法的效果。它的每一列是样本的预测分类，每一行是样本的真实分类（反过来也可以），顾名思义，它反映了分类结果的混淆程度。混淆矩阵$i$行$j$列的原始是原本是类别$i$却被分为类别$j$的样本个数，计算完之后还可以对之进行可视化： 多分类问题对于多分类问题，或者在二分类问题中，我们有时候会有多组混淆矩阵，例如：多次训练或者在多个数据集上训练的结果，那么估算全局性能的方法有两种，分为宏平均（macro-average）和微平均（micro-average）。简单理解，宏平均就是先算出每个混淆矩阵的P值和R值，然后取得平均P值macro-P和平均R值macro-R，再算出$Fβ$或$F1$，而微平均则是计算出混淆矩阵的平均TP、FP、TN、FN，接着进行计算P、R，进而求出$Fβ$或$F1$。其它分类指标同理，均可以通过宏平均/微平均计算得出。 \operatorname{macro}P=\frac{1}{n} \sum_{i=1}^{n} P_{i} \operatorname{macro}R=\frac{1}{n} \sum_{i=1}^{n} R_{i} \operatorname{macro} F 1=\frac{2 \times \operatorname{macro} P \times \operatorname{macro} R}{\operatorname{macro} P+\operatorname{macro}R} \operatorname{micro} P=\frac{\overline{T P}}{\overline{T P}+\overline{F P}} \operatorname{micro}R=\frac{\overline{T P}}{\overline{T P}+\overline{F N}} \operatorname{micro} F 1=\frac{2 \times \operatorname{micro} P \times \operatorname{micro} R}{\operatorname{micro} P+\operatorname{micro}R}需要注意的是，在多分类任务场景中，如果非要用一个综合考量的metric的话，宏平均会比微平均更好一些，因为宏平均受稀有类别影响更大。宏平均平等对待每一个类别，所以它的值主要受到稀有类别的影响，而微平均平等考虑数据集中的每一个样本，所以它的值受到常见类别的影响比较大。]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>机器学习</tag>
        <tag>深度学习</tag>
        <tag>分类算法</tag>
        <tag>评价指标</tag>
        <tag>分类算法评价指标</tag>
        <tag>准确率、精确率、召回率</tag>
        <tag>P-R曲线</tag>
        <tag>F1指数</tag>
        <tag>ROC</tag>
        <tag>AUC</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Git常用操作指南]]></title>
    <url>%2FGit-tutorial%2F</url>
    <content type="text"><![CDATA[前言因为工作需求，最近又重新温习了一下Git操作，遂总结了一篇Git常用操作指南，方便日后学习查阅，本博客精简提炼了在开发过程中Git经常用到的核心命令，主要参考了《廖雪峰老师的Git教程》，希望对大家学习使用Git能带来帮助。 Git简介Git是Linux之父Linus的第二个伟大的作品，它最早是在Linux上开发的，被用来管理Linux核心的源代码。后来慢慢地有人将其移植到了Unix、Windows、Max OS等操作系统中。 Git是一个分布式的版本控制系统，与集中式的版本控制系统不同的是，每个人都工作在通过克隆建立的本地版本库中。也就是说每个人都拥有一个完整的版本库，查看提交日志、提交、创建里程碑和分支、合并分支、回退等所有操作都直接在本地完成而不需要网络连接。 对于Git仓库来说，每个人都有一个独立完整的仓库，所谓的远程仓库或是服务器仓库其实也是一个仓库，只不过这台主机24小时运行，它是一个稳定的仓库，供他人克隆、推送，也从服务器仓库中拉取别人的提交。 Git是目前世界上最先进的分布式版本控制系统。 安装之后第一步安装完成后，还需要最后一步设置，在命令行输入： 12$ git config --global user.name "Your Name"$ git config --global user.email "email@example.com" 因为Git是分布式版本控制系统，所以，每个机器都必须配置用户信息：你的名字和Email地址。 注意git config命令的--global参数，用了这个参数，表示你这台机器上所有的Git仓库都会使用这个配置，当然也可以对某个仓库指定不同的用户名和Email地址。 创建版本库本地仓库版本库又名仓库，英文名repository，你可以简单理解成一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改、删除，Git都能跟踪，以便任何时刻都可以追踪历史，或者在将来某个时刻可以“还原”。 所以，创建一个版本库非常简单，首先，选择一个合适的地方，创建一个空目录： 123456$ mkdir learngit$ cd learngit$ pwdPath----D:\Blog\tmp\learngit 第二步，通过git init命令把这个目录变成Git可以管理的仓库： 12$ git initInitialized empty Git repository in D:/Blog/tmp/learngit/.git/ 远程仓库创建SSH KeyGit支持多种协议，包括https，但通过ssh支持的原生git协议速度最快。由于本地Git仓库和GitHub仓库之间的传输是通过SSH加密的，所以，需要在关联远程仓库前需要配置SSH Key至Github设置中，这样远程仓库才允许本机对远程仓库的拉去/推送操作。 打开Shell，进入到”~/.ssh“目录下，运行”ls“命令看看这个目录下有没有id_rsa和id_rsa.pub这两个文件，如果已经有了，可直接跳到下一步。 如果没有，则执行： 1$ ssh-keygen -t rsa -C "youremail@example.com" 一路回车即可。执行命令后，我们再进入到”~/.ssh“目录下，运行”ls“命令，可以看到里面有id_rsa和id_rsa.pub两个文件，这两个就是SSH Key的秘钥对，id_rsa是私钥，不能泄露出去，id_rsa.pub是公钥，可以放心地告诉任何人。 打开“Account settings”，“SSH Keys”页面，然后，点“New SSH Key”，填上任意Title，在Key文本框里粘贴id_rsa.pub文件的内容（Win 10 下可使用”type ~/.ssh/id_rsa.pub“命令查看公钥文件内容）： 点击“Add SSH Key”之后，就可以看到你的公钥已经加入到了你的Github仓库配置中。 添加远程库首先，登陆GitHub，然后，在右上角找到“Create a new repo”按钮，创建一个新的仓库： 在Repository name填入learngit，其他保持默认设置，点击“Create repository”按钮，就成功地创建了一个新的Git仓库： 这样就成功创建了一个空白的远程仓库，那么如何将这个远程仓库与本地仓库进行关联呢？ 我们根据Git所给出的提示可知，可以在本地创建一个新仓库对远程仓库进行关联，也可以对本地已有仓库进行关联。 关联新仓库123456echo "# learngit" &gt;&gt; README.mdgit initgit add README.mdgit commit -m "first commit"git remote add origin git@github.com:guoyaohua/learngit.gitgit push -u origin master 关联已有仓库12git remote add origin git@github.com:guoyaohua/learngit.gitgit push -u origin master 我们可以使用上文在本地初始化的“learngit”仓库。（注意：本地仓库和远程仓库可以不同名，本文只是为了写教程设置为相同名称。） 我们再刷新下Github Code界面，发现新加入的README.md文件已经推送到了远程仓库中。 版本控制工作区和暂存区工作区（Working Directory）就是你在电脑里能看到的目录，比如我们刚刚创建的learngit文件夹就是一个工作区： 版本库（Repository）工作区有一个隐藏目录.git，这个不算工作区，而是Git的版本库。 Git的版本库里存了很多东西，其中最重要的就是称为Stage（或者叫Index）的暂存区，还有Git为我们自动创建的第一个分支master，以及指向master的一个指针叫HEAD。 分支和HEAD的概念本文后面再详细说明。 我们把文件往Git版本库里添加的时候，是分两步执行的： 第一步是用git add把文件添加进去，实际上就是把文件修改添加到暂存区； 第二步是用git commit提交更改，实际上就是把暂存区的所有内容提交到当前分支。 因为我们创建Git版本库时，Git自动为我们创建了唯一一个master分支，所以现在，git commit就是往master分支上提交更改。 你可以简单理解为，需要提交的文件修改通通放到暂存区，然后，一次性提交暂存区的所有修改。 使用git status命令可以查看当前仓库的状态。 版本回退Git版本控制可以理解为，我们再编写代码的过程中，会对code进行多次修改，每当你觉得文件修改到一定程度的时候，就可以“保存一个快照”，这个快照在Git中被称为commit。一旦你把文件改乱了，或者误删了文件，还可以从最近的一个commit恢复，然后继续工作，而不是把几个月的工作成果全部丢失。 在实际工作中，我们用git log命令查看我们提交的历史记录： 123456789101112131415161718$ git logcommit 1094adb7b9b3807259d8cb349e7df1d4d6477073 (HEAD -&gt; master)Author: Yaohua Guo &lt;guo.yaohua@foxmail.com&gt;Date: Fri May 18 21:06:15 2018 +0800 append GPLcommit e475afc93c209a690c39c13a46716e8fa000c366Author: Yaohua Guo &lt;guo.yaohua@foxmail.com&gt;Date: Fri May 18 21:03:36 2018 +0800 add distributedcommit eaadf4e385e865d25c48e7ca9c8395c3f7dfaef0Author: Yaohua Guo &lt;guo.yaohua@foxmail.com&gt;Date: Fri May 18 20:59:18 2018 +0800 wrote a readme file Git中，commit id是一个使用SHA1计算出来的一个非常大的数字，用十六进制表示，commit后面的那一串十六进制数字就是每一次提交的版本号，我们可以通过git log命令看到每次提交的版本号、用户名、日期以及版本描述等信息。 我们可以使用git reset命令进行版本回退操作。 1$ git reset --hard HEAD^ 在Git中，用HEAD表示当前版本，上一个版本就是HEAD^ ，上上一个版本就是HEAD^^ ，以此类推，如果需要回退几十个版本，写几十个^容易数不过来，所以可以写，例如回退30个版本为：HEAD~30。 如果回退完版本又后悔了，想恢复，也是可以的，使用如下即可： 1$ git reset --hard commit_id 不过当我们执行git reset进行版本回退之后，之前最新的版本号无法通过git log查询到，此时需要使用git reflog命令查询Git的操作记录，我们可以从该记录中找到之前的commit id信息。 12345$ git refloge475afc HEAD@&#123;1&#125;: reset: moving to HEAD^1094adb (HEAD -&gt; master) HEAD@&#123;2&#125;: commit: append GPLe475afc HEAD@&#123;3&#125;: commit: add distributedeaadf4e HEAD@&#123;4&#125;: commit (initial): wrote a readme file 在Git中，版本回退速度非常快，因为Git在内部有个指向当前版本的HEAD指针，当你回退版本的时候，Git仅仅是把HEAD从指向回退的版本，然后顺便刷新工作区文件。 重置命令重置命令的作用是将当前的分支重设（reset）到指定的&lt;commit&gt;或者HEAD（默认是HEAD，即最新的一次提交），并且根据[mode]有可能更新Index和Working directory（默认是mixed）。 1$ git reset [--hard|soft|mixed|merge|keep] [commit|HEAD] –hard：重设“暂存区”和“工作区”，从&lt;commit&gt;以来在工作区中的任何改变都被丢弃，并把HEAD指向&lt;commit&gt;。（彻底回退到某个版本，本地的源码也会变为上一个版本的内容。） –soft：“工作区”中的内容不作任何改变，HEAD指向&lt;commit&gt;，自从&lt;commit&gt;以来的所有改变都会回退到“暂存区”中，显示在git status的“Changes to be committed”中。（回退到某个版本，只回退了commit的信息。如果还要提交，直接commit即可。） –mixed：仅重设“暂存区”，并把HEAD指向&lt;commit&gt;，但是不重设“工作区”，本地文件修改不受影响。这个模式是默认模式，即当不显示告知git reset模式时，会使用mixed模式。这个模式的效果是，工作区中文件的修改都会被保留，不会丢弃，但是也不会被标记成“Changes to be committed”，但是会提示文件未被更新。（回退到某个版本，只保留源码，回退commit和index信息） 文件粒度操作需要注意的是在mixed模式下进行reset操作时可以是全局性重置，也可以是文件粒度重置，区别在于二者作用域不同，文件粒度只会使对应文件的暂存区状态变为指定commit时该文件的暂存区状态，并且不会改变版本库状态，即HEAD指针不会改变，我们看一下效果。 首先我们新建两个文件进行两次提交，可以看到目前HEAD指向最新一次提交“text2”。 我们对“file1.txt”进行reset操作，令其重置为“text1”状态。 并且我们通过git log命令可发现，此时HEAD指针并没有改变，还是指向最新一次提交“Text 2”，可知文件粒度的reset --mixed不改变版本库HEAD指针状态。 对于soft和hard模式则无法进行文件粒度操作。 Reset 常用示例 回退add操作 123$ git add test$ git reset HEAD test # HEAD指的是当前指向的版本号，可以将HEAD还成任意想回退的版本号 可以将test从“已暂存”状态（Index区）回滚到指定Commit时暂存区的状态。 回退最后一次提交 123$ git add test$ git commit -m "Add test"$ git reset --soft HEAD^ 可以将test从“已提交”状态变为“已暂存”状态。 回退最近几次提交，并把这几次提交放到新分支上 123$ git branch topic # 已当前分支为基础，新建分支topic$ git reset --hard HEAD~2 # 在当前分支上回滚提交$ git checkout topic 通过临时分支来保留提交，然后在当前分支上做硬回滚。 将本地的状态回退到和远程一样 1$ git reset --hard origin/devlop 回退到某个版本提交 1$ git reset 497e350 当前HEAD会指向“497e350”，暂存区中的状态会恢复到提交“497e350”时暂存区的状态。 撤销修改当我们因为一些原因想要丢弃工作区某些文件修改时，可以使用“git checkout -- &lt;file&gt;”命令，该命令仅会恢复工作区文件状态，不会对版本库有任何改动。 命令git checkout -- file1.txt意思就是，把file1.txt文件在工作区的修改全部撤销，这里有两种情况： 一种是file1.txt自修改后还没有被放到暂存区，现在，撤销修改就回到和版本库一模一样的状态； 一种是file1.txt已经添加到暂存区后，又作了修改，现在，撤销修改就回到添加到暂存区后的状态。 总之，就是让这个文件回到最近一次git commit或git add时的状态。 删除文件在Git中，删除也是一个修改操作，我们实战一下，先添加一个新文件test.txt到Git并且提交： 一般情况下，你通常直接在文件管理器中把没用的文件删了，或者用rm命令删了： 1$ rm test.txt 这个时候，Git知道你删除了文件，因此，工作区和版本库就不一致了，git status命令会立刻告诉你哪些文件被删除了： 123456789$ git statusOn branch masterChanges not staged for commit: (use "git add/rm &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) deleted: test.txtno changes added to commit (use "git add" and/or "git commit -a") 现在你有两个选择，一是确实要从版本库中删除该文件，那就用命令git rm删掉，并且git commit： 1234567$ git rm test.txtrm 'test.txt'$ git commit -m "remove test.txt"[master d46f35e] remove test.txt 1 file changed, 1 deletion(-) delete mode 100644 test.txt 现在，文件就从版本库中被删除了。 提示：先手动删除文件，然后使用git rm &lt;file&gt;和git add &lt;file&gt;效果是一样的。 另一种情况是删错了，因为版本库里还有呢，所以可以很轻松地把误删的文件恢复到最新版本： 1$ git checkout -- test.txt git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原”。 注意：从来没有被添加到版本库就被删除的文件，是无法恢复的！ 分支管理创建与合并分支在上文“版本回退”里，我们已经知道，每次提交，Git都把它们串成一条时间线，这条时间线就是一个分支。截止到目前，只有一条时间线，在Git里，这个分支叫主分支，即master分支。HEAD严格来说不是指向提交，而是指向master，master才是指向提交的，所以，HEAD指向的就是当前分支。 一开始的时候，master分支是一条线，Git用master指向最新的提交，再用HEAD指向master，就能确定当前分支，以及当前分支的提交点： 每次提交，master分支都会向前移动一步，这样，随着你不断提交，master分支的线也越来越长。 当我们创建新的分支，例如dev时，Git新建了一个指针叫dev，指向master相同的提交，再把HEAD指向dev，就表示当前分支在dev上： Git创建一个分支很快，因为除了增加一个dev指针，改改HEAD的指向，工作区的文件都没有任何变化。 不过，从现在开始，对工作区的修改和提交就是针对dev分支了，比如新提交一次后，dev指针往前移动一步，而master指针不变： 假如我们在dev上的工作完成了，就可以把dev合并到master上。Git怎么合并呢？最简单的方法，就是直接把master指向dev的当前提交，就完成了合并： 所以Git合并分支也很快！就改改指针，工作区内容也不变！ 合并完分支后，甚至可以删除dev分支。删除dev分支就是把dev指针给删掉，删掉后，我们就剩下了一条master分支： 下面开始实战。 首先，我们创建dev分支，然后切换到dev分支： 12$ git checkout -b devSwitched to a new branch 'dev' git checkout命令加上-b参数表示创建并切换，相当于以下两条命令： 123$ git branch dev # 创建dev分支$ git checkout dev # 切换到dev分支Switched to branch 'dev' 然后，用git branch命令查看当前分支： 123$ git branch* dev master git branch命令会列出所有分支，当前分支前面会标一个*号。 然后，我们就可以在dev分支上正常提交，比如对readme.txt做个修改，加上一行： 1Creating a new branch is quick. 然后提交： 1234$ git add readme.txt $ git commit -m "branch test"[dev b17d20e] branch test 1 file changed, 1 insertion(+) 现在，dev分支的工作完成，我们就可以切换回master分支： 12$ git checkout masterSwitched to branch 'master' 切换回master分支后，再查看一个readme.txt文件，刚才添加的内容不见了！因为那个提交是在dev分支上，而master分支此刻的提交点并没有变： 现在，我们把dev分支的工作成果合并到master分支上： 12345$ git merge devUpdating d46f35e..b17d20eFast-forward readme.txt | 1 + 1 file changed, 1 insertion(+) git merge命令用于合并指定分支到当前分支。合并后，再查看readme.txt的内容，就可以看到，和dev分支的最新提交是完全一样的。 注意到上面的Fast-forward信息，Git告诉我们，这次合并是“快进模式”，也就是直接把master指向dev的当前提交，所以合并速度非常快。 当然，也不是每次合并都能Fast-forward，我们后面会讲其他方式的合并。 合并完成后，就可以放心地删除dev分支了： 12$ git branch -d devDeleted branch dev (was b17d20e). 删除后，查看branch，就只剩下master分支了： 12$ git branch* master 因为创建、合并和删除分支非常快，所以Git鼓励你使用分支完成某个任务，合并后再删掉分支，这和直接在master分支上工作效果是一样的，但过程更安全。 解决冲突在真正开发过程中，合并分支经常会遇到分支冲突的情况，无法直接合并，我们来模拟一下这个场景。 准备新的feature1分支，继续我们的新分支开发： 12$ git checkout -b feature1Switched to a new branch 'feature1' 修改readme.txt最后一行，改为： 1Creating a new branch is quick AND simple. 在feature1分支上提交： 12345$ git add readme.txt$ git commit -m "AND simple"[feature1 14096d0] AND simple 1 file changed, 1 insertion(+), 1 deletion(-) 切换到master分支： 1234$ git checkout masterSwitched to branch 'master'Your branch is ahead of 'origin/master' by 1 commit. (use "git push" to publish your local commits) Git还会自动提示我们当前master分支比远程的master分支要超前1个提交。 在master分支上把readme.txt文件的最后一行改为： 1Creating a new branch is quick &amp; simple. 提交： 1234$ git add readme.txt $ git commit -m "&amp; simple"[master 5dc6824] &amp; simple 1 file changed, 1 insertion(+), 1 deletion(-) 现在，master分支和feature1分支各自都分别有新的提交，变成了这样： 这种情况下，Git无法执行“快速合并(Fast-forward)”，只能试图把各自的修改合并起来，但这种合并就可能会有冲突，我们试试看： 1234$ git merge feature1Auto-merging readme.txtCONFLICT (content): Merge conflict in readme.txtAutomatic merge failed; fix conflicts and then commit the result. Git告诉我们，readme.txt文件存在冲突，必须手动解决冲突后再提交。git status也可以告诉我们冲突的文件： 123456789101112131415$ git statusOn branch masterYour branch is ahead of 'origin/master' by 2 commits. (use "git push" to publish your local commits)You have unmerged paths. (fix conflicts and run "git commit") (use "git merge --abort" to abort the merge)Unmerged paths: (use "git add &lt;file&gt;..." to mark resolution) both modified: readme.txtno changes added to commit (use "git add" and/or "git commit -a") 我们可以直接查看readme.txt的内容： 123456789Git is a distributed version control system.Git is free software distributed under the GPL.Git has a mutable index called stage.Git tracks changes of files.&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEADCreating a new branch is quick &amp; simple.=======Creating a new branch is quick AND simple.&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature1 Git用&lt;&lt;&lt;&lt;&lt;&lt;&lt;，=======，&gt;&gt;&gt;&gt;&gt;&gt;&gt;标记出不同分支的内容，我们修改如下后保存： 1Creating a new branch is quick and simple. 再提交： 123$ git add readme.txt $ git commit -m "conflict fixed"[master cf810e4] conflict fixed 现在，master分支和feature1分支变成了下图所示： 用带参数的git log也可以看到分支的合并情况： 1234567891011121314$ git log --graph --pretty=oneline --abbrev-commit* cf810e4 (HEAD -&gt; master) conflict fixed|\ | * 14096d0 (feature1) AND simple* | 5dc6824 &amp; simple|/ * b17d20e branch test* d46f35e (origin/master) remove test.txt* b84166e add test.txt* 519219b git tracks changes* e43a48b understand how stage works* 1094adb append GPL* e475afc add distributed* eaadf4e wrote a readme file 最后，删除feature1分支： 12$ git branch -d feature1Deleted branch feature1 (was 14096d0). 工作完成。 分支管理策略通常，合并分支时，如果可能，Git会用Fast forward模式，但这种模式下，删除分支后，会丢掉分支信息。 如果要强制禁用Fast forward模式，Git就会在merge时生成一个新的commit，这样，从分支历史上就可以看出分支信息。 下面我们实战一下--no-ff方式的git merge： 首先，仍然创建并切换dev分支： 12$ git checkout -b devSwitched to a new branch 'dev' 修改readme.txt文件，并提交一个新的commit： 1234$ git add readme.txt $ git commit -m "add merge"[dev f52c633] add merge 1 file changed, 1 insertion(+) 现在，我们切换回master： 12$ git checkout masterSwitched to branch 'master' 准备合并dev分支，请注意--no-ff参数，表示禁用Fast forward： 1234$ git merge --no-ff -m "merge with no-ff" devMerge made by the 'recursive' strategy. readme.txt | 1 + 1 file changed, 1 insertion(+) 因为本次合并要创建一个新的commit，所以加上-m参数，把commit描述写进去。 合并后，我们用git log看看分支历史： 1234567$ git log --graph --pretty=oneline --abbrev-commit* e1e9c68 (HEAD -&gt; master) merge with no-ff|\ | * f52c633 (dev) add merge|/ * cf810e4 conflict fixed... 可以看到，不使用Fast forward模式，merge后就像这样： 分支策略在实际开发中，我们应该按照几个基本原则进行分支管理： 首先，master分支应该是非常稳定的，也就是仅用来发布新版本，平时不能在上面干活； 那在哪干活呢？干活都在dev分支上，也就是说，dev分支是不稳定的，到某个时候，比如1.0版本发布时，再把dev分支合并到master上，在master分支发布1.0版本； 你和团队同事每个人都在dev分支上干活，每个人都有自己的分支，时不时地往dev分支上合并就可以了。 所以，团队合作的分支看起来就像这样： 状态存储当我们在开发过程中，经常遇到这样的情况，我们需要暂时放下手中的工作，切换到其他分支进行开发，例如当我们在dev分支进行程序2.0版本开发时，发现1.0版本的程序出现了bug，必须立刻进行修复，但是在目前的dev分支我们可能已经做了很多修改，暂存区可能有了暂存状态，甚至可能在开发过程中在dev分支进行了多次commit，这时如果我们想切换回master分支，进行bug修复，这时就需要使用到git stash命令存储原分支当前的状态。 在讲解git stash之前，我们先考虑两种场景： 第一种就是我们未在dev分支进行任何提交，此时HEAD指针指向dev，dev和master指向同一次commit，如下图： 我们可能在dev的工作区做了很多修改，也将部分修改状态加入了暂存区（即进行了git add操作），这时我们尝试一下直接使用git checkout命令切换分支。 此时，Git状态如下： 我们修改“file1.txt”和“file2.txt”的内容，并将“file1.txt”的改动加入暂存区。 此时可看出工作区和暂存区就都有改变，但HEAD指针指向的dev与master为同一个commit节点。 这时我们执行git checkout master命令尝试切换分支。 可以看出，成功切换到了master分支上，而且工作区和暂存区的状态依旧保留。 我们再考虑一个场景，在dev分支开发时，进行了一次提交，此时HEAD指向dev分支，dev分支超前master分支一次commit，具体见下图： 如果此时我们工作区或暂存区有未提交更改时，就无法进行分支切换操作（如果没有未提交修改的话当然可以进行分支切换操作）。 我想这时大家就会有一个疑问，为什么两种状态下我们都修改了暂存区和工作区的状态，但是一个可以切换分支并且保留工作区、暂存区状态，而另一种状态就无法切换分支呢？ 我起初在遇到这个问题的时候也是很诧异，在网上搜索了好多资料，依旧没有查到有价值的信息。 这时我们就应该从Git的原理来进行分析了，Git在进行版本控制时，记录的并不是文件本身的信息，而是文件的修改状态，例如我们再一个10000行代码的文件中，新加入了一行代码进行，Git并不是将最新的10001行代码作为备份，而是仅仅记录了新旧文件之间的差异，即在哪个位置修改了什么内容（修改包括：增加、删除、修改等）。 我们来分析一下上问题到的第一种场景：我们未在dev分支进行任何提交，此时HEAD指针指向dev，dev和master指向同一次commit。 虽然我们再dev分支的工作区和暂存区做了修改，这些修改都是基于dev指向的commit而言的，而且此时dev和master指向同一个commit，所以，该场景下，dev分支工作区和暂存区的修改依旧适用于master分支，所以可以成功切换分支。 而第二种场景：在dev分支开发时，进行了一次提交，此时HEAD指向dev分支，dev分支超前master分支一次commit。 这时，dev工作区和暂存区的状态是基于最新的dev指向的commit而言的，已经不能应用于master指向的commit了，所以在进行切换分支时，提示报错。 应用实例软件开发中，bug就像家常便饭一样。有了bug就需要修复，在Git中，由于分支是如此的强大，所以，每个bug都可以通过一个新的临时分支来修复，修复后，合并分支，然后将临时分支删除。 当你接到一个修复一个代号101的bug的任务时，很自然地，你想创建一个分支issue-101来修复它，但是，当前正在dev上进行的工作还没有提交： 123456789101112$ git statusOn branch devChanges to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: hello.pyChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: readme.txt 并不是你不想提交，而是工作只进行到一半，还没法提交，预计完成还需1天时间。但是，必须在两个小时内修复该bug，怎么办？ 幸好，Git还提供了一个stash功能，可以把当前工作现场“储藏”起来，等以后恢复现场后继续工作： 12$ git stashSaved working directory and index state WIP on dev: f52c633 add merge 现在，用git status查看工作区，就是干净的（除非有没有被Git管理的文件），因此可以放心地创建分支来修复bug。 首先确定要在哪个分支上修复bug，假定需要在master分支上修复，就从master创建临时分支： 1234567$ git checkout masterSwitched to branch 'master'Your branch is ahead of 'origin/master' by 6 commits. (use "git push" to publish your local commits)$ git checkout -b issue-101Switched to a new branch 'issue-101' 现在修复bug，需要把“Git is free software …”改为“Git is a free software …”，然后提交： 1234$ git add readme.txt $ git commit -m "fix bug 101"[issue-101 4c805e2] fix bug 101 1 file changed, 1 insertion(+), 1 deletion(-) 修复完成后，切换到master分支，并完成合并，最后删除issue-101分支： 123456789$ git checkout masterSwitched to branch 'master'Your branch is ahead of 'origin/master' by 6 commits. (use "git push" to publish your local commits)$ git merge --no-ff -m "merged bug fix 101" issue-101Merge made by the 'recursive' strategy. readme.txt | 2 +- 1 file changed, 1 insertion(+), 1 deletion(-) 修复好BUG之后，就可以返回原分支继续之前的工作了。 123456$ git checkout devSwitched to branch 'dev'$ git statusOn branch devnothing to commit, working tree clean 工作区是干净的，刚才的工作现场存到哪去了？用git stash list命令看看： 12$ git stash liststash@&#123;0&#125;: WIP on dev: f52c633 add merge 工作现场还在，Git把stash内容存在某个地方了，但是需要恢复一下，有两个办法： 一是用git stash apply恢复，但是恢复后，stash内容并不删除，你需要用git stash drop来删除； 另一种方式是用git stash pop，恢复的同时把stash内容也删了： 1234567891011121314$ git stash popOn branch devChanges to be committed: (use "git reset HEAD &lt;file&gt;..." to unstage) new file: hello.pyChanges not staged for commit: (use "git add &lt;file&gt;..." to update what will be committed) (use "git checkout -- &lt;file&gt;..." to discard changes in working directory) modified: readme.txtDropped refs/stash@&#123;0&#125; (5d677e2ee266f39ea296182fb2354265b91b3b2a) 再用git stash list查看，就看不到任何stash内容了： 1$ git stash list 你可以多次stash，恢复的时候，先用git stash list查看，然后恢复指定的stash，用命令： 1$ git stash apply stash@&#123;0&#125; 多人协作当你从远程仓库克隆时，实际上Git自动把本地的master分支和远程的master分支对应起来了，并且，远程仓库的默认名称是origin。 用git remote -v查看远程库的详细信息： 123$ git remote -vorigin git@github.com:guoyaohua/learngit.git (fetch)origin git@github.com:guoyaohua/learngit.git (push) 上面显示了可以抓取和推送的origin的地址。如果没有推送权限，就看不到push的地址。 推送分支推送分支，就是把该分支上的所有本地提交推送到远程库。推送时，要指定本地分支，这样，Git就会把该分支推送到远程库对应的远程分支上： 1$ git push origin master 如果要推送其他分支，比如dev，就改成： 1$ git push origin dev 但是，并不是一定要把本地分支往远程推送，那么，哪些分支需要推送，哪些不需要呢？ master分支是主分支，因此要时刻与远程同步； dev分支是开发分支，团队所有成员都需要在上面工作，所以也需要与远程同步； bug分支只用于在本地修复bug，就没必要推到远程了，除非老板要看看你每周到底修复了几个bug； feature分支是否推到远程，取决于你是否和你的小伙伴合作在上面开发。 总之，就是在Git中，分支完全可以在本地自己藏着玩，是否推送，视你的心情而定！ 抓取分支多人协作时，大家都会往master和dev分支上推送各自的修改。 现在，模拟一个你的同事，可以在另一台电脑（注意要把SSH Key添加到GitHub）或者同一台电脑的另一个目录下克隆： 1234567$ git clone git@github.com:guoyaohua/learngit.gitCloning into 'learngit'...remote: Counting objects: 40, done.remote: Compressing objects: 100% (21/21), done.remote: Total 40 (delta 14), reused 40 (delta 14), pack-reused 0Receiving objects: 100% (40/40), done.Resolving deltas: 100% (14/14), done. 当你的同事从远程库clone时，默认情况下，你的同事只能看到本地的master分支。不信可以用git branch命令看看： 12$ git branch* master 现在，你的同事要在dev分支上开发，就必须创建远程origin的dev分支到本地，于是他用这个命令创建本地dev分支： 1$ git checkout -b dev origin/dev 现在，他就可以在dev上继续修改，然后，时不时地把dev分支push到远程： 123456789101112131415$ git add env.txt$ git commit -m "add env"[dev 7a5e5dd] add env 1 file changed, 1 insertion(+) create mode 100644 env.txt$ git push origin devCounting objects: 3, done.Delta compression using up to 4 threads.Compressing objects: 100% (2/2), done.Writing objects: 100% (3/3), 308 bytes | 308.00 KiB/s, done.Total 3 (delta 0), reused 0 (delta 0)To github.com:michaelliao/learngit.git f52c633..7a5e5dd dev -&gt; dev 你的同事已经向origin/dev分支推送了他的提交，而碰巧你也对同样的文件作了修改，并试图推送： 123456789101112131415161718$ type env.txtenv$ git add env.txt$ git commit -m "add new env"[dev 7bd91f1] add new env 1 file changed, 1 insertion(+) create mode 100644 env.txt$ git push origin devTo github.com:michaelliao/learngit.git ! [rejected] dev -&gt; dev (non-fast-forward)error: failed to push some refs to 'git@github.com:guoyaohua/learngit.git'hint: Updates were rejected because the tip of your current branch is behindhint: its remote counterpart. Integrate the remote changes (e.g.hint: 'git pull ...') before pushing again.hint: See the 'Note about fast-forwards' in 'git push --help' for details. 推送失败，因为你的同事的最新提交和你试图推送的提交有冲突，解决办法也很简单，Git已经提示我们，先用git pull把最新的提交从origin/dev抓下来，然后，在本地合并，解决冲突，再推送： 12345678910$ git pullThere is no tracking information for the current branch.Please specify which branch you want to merge with.See git-pull(1) for details. git pull &lt;remote&gt; &lt;branch&gt;If you wish to set tracking information for this branch you can do so with: git branch --set-upstream-to=origin/&lt;branch&gt; dev git pull也失败了，原因是没有指定本地dev分支与远程origin/dev分支的链接，根据提示，设置dev和origin/dev的链接： 12$ git branch --set-upstream-to=origin/dev devBranch 'dev' set up to track remote branch 'dev' from 'origin'. 再pull： 1234$ git pullAuto-merging env.txtCONFLICT (add/add): Merge conflict in env.txtAutomatic merge failed; fix conflicts and then commit the result. 这回git pull成功，但是合并有冲突，需要手动解决，解决的方法和分支管理中的解决冲突完全一样。解决后，提交，再push： 1234567891011$ git commit -m "fix env conflict"[dev 57c53ab] fix env conflict$ git push origin devCounting objects: 6, done.Delta compression using up to 4 threads.Compressing objects: 100% (4/4), done.Writing objects: 100% (6/6), 621 bytes | 621.00 KiB/s, done.Total 6 (delta 0), reused 0 (delta 0)To git@github.com:guoyaohua/learngit.git 7a5e5dd..57c53ab dev -&gt; dev 因此，多人协作的工作模式通常是这样： 首先，可以试图用git push origin &lt;branch-name&gt;推送自己的修改； 如果推送失败，则因为远程分支比你的本地更新，需要先用git pull试图合并； 如果合并有冲突，则解决冲突，并在本地提交； 没有冲突或者解决掉冲突后，再用git push origin &lt;branch-name&gt;推送就能成功！ 如果git pull提示no tracking information，则说明本地分支和远程分支的链接关系没有创建，用命令git branch --set-upstream-to &lt;branch-name&gt; origin/&lt;branch-name&gt;。 这就是多人协作的工作模式，一旦熟悉了，就非常简单。 Rebasegit rebase和git merge做的事其实是一样的。它们都被设计来将一个分支的更改并入另一个分支，只不过方式有些不同。 git rebase用于把一个分支的修改合并到当前分支。 假设你现在基于远程分支”origin”，创建一个叫”mywork”的分支。 1$ git checkout -b mywork origin 假设远程分支”origin”已经有了2个提交，如图： 现在我们在这个分支做一些修改，然后生成两个提交(commit)。 但是与此同时，有些人也在”origin”分支上做了一些修改并且做了提交了. 这就意味着”origin”和”mywork”这两个分支各自”前进”了，它们之间”分叉”了。 在这里，你可以用“pull”命令把“origin”分支上的修改拉下来并且和你的修改合并； 结果看起来就像一个新的”合并的提交”(merge commit): 但是，如果你想让“mywork”分支历史看起来像没有经过任何合并一样，你也许可以用 git rebase： 12$ git checkout mywork$ git rebase origin 这些命令会把你的”mywork”分支里的每个提交(commit)取消掉，并且把它们临时保存为补丁(patch)(这些补丁放到”.git/rebase”目录中)，然后把”mywork”分支更新为最新的”origin”分支，最后把保存的这些补丁应用到”mywork”分支上。 当”mywork”分支更新之后，它会指向这些新创建的提交(commit)，而那些老的提交会被丢弃。 如果运行垃圾收集命令(pruning garbage collection)，这些被丢弃的提交就会删除。 现在我们可以看一下用merge和用rebase所产生的历史的区别： 当我们使用git log来参看commit时，其commit的顺序也有所不同。 假设C3提交于9:00AM，C5提交于10:00AM，C4提交于11:00AM，C6提交于12:00AM， 对于使用git merge来合并所看到的commit的顺序（从新到旧）是： C7，C6，C4，C5，C3，C2，C1 对于使用git rebase来合并所看到的commit的顺序（从新到旧）是： C7，C6’，C5’，C4，C3，C2，C1 因为C6’提交只是C6提交的克隆，C5’提交只是C5提交的克隆， 从用户的角度看使用git rebase来合并后所看到的commit的顺序（从新到旧）是： C7，C6，C5，C4，C3，C2，C1 另外，我们在使用git pull命令的时候，可以使用--rebase参数，即git pull --rebase，这里Git会把你的本地当前分支里的每个提交(commit)取消掉，并且把它们临时保存为补丁(patch)(这些补丁放到”.git/rebase”目录中),然后把分支更新 为最新的”origin”分支，最后把保存的这些补丁应用到分支上。 解决冲突在rebase的过程中，也许会出现冲突(conflict)。在这种情况，Git会停止rebase并会让你去解决冲突。rebase和merge的另一个区别是rebase的冲突是一个一个解决，如果有十个冲突，在解决完第一个冲突后，用”git add“命令去更新这些内容的索引(index)，然后，你无需执行 git-commit，只要执行： 12$ git add -u $ git rebase --continue 继续后才会出现第二个冲突，直到所有冲突解决完，而merge是所有的冲突都会显示出来。 在任何时候，你可以用--abort参数来终止rebase的行动，并且”mywork” 分支会回到rebase开始前的状态。 1$ git rebase --abort 所以rebase的工作流就是 123456789git rebase while(存在冲突) &#123; git status # 找到当前冲突文件，编辑解决冲突 git add -u git rebase --continue if( git rebase --abort ) break; &#125; 最后冲突全部解决，rebase成功。 标签管理发布一个版本时，我们通常先在版本库中打一个标签（tag），这样，就唯一确定了打标签时刻的版本。将来无论什么时候，取某个标签的版本，就是把那个打标签的时刻的历史版本取出来。所以，标签也是版本库的一个快照。 Git的标签虽然是版本库的快照，但其实它就是指向某个commit的指针（跟分支很像，但是分支可以移动，标签不能移动），所以，创建和删除标签都是瞬间完成的。 Git有commit，为什么还要引入tag？ “请把上周一的那个版本打包发布，commit号是6a5819e…” “一串乱七八糟的数字不好找！” 如果换一个办法： “请把上周一的那个版本打包发布，版本号是v1.2” “好的，按照tag v1.2查找commit就行！” 所以，tag就是一个让人容易记住的有意义的名字，它跟某个commit绑在一起。 创建标签在Git中打标签非常简单，首先，切换到需要打标签的分支上： 12345$ git branch* dev master$ git checkout masterSwitched to branch 'master' 然后，敲命令git tag &lt;name&gt;就可以打一个新标签： 1$ git tag v1.0 可以用命令git tag查看所有标签： 12$ git tagv1.0 默认标签是打在最新提交的commit上的。有时候，如果忘了打标签，比如，现在已经是周五了，但应该在周一打的标签没有打，怎么办？ 方法是找到历史提交的commit id，然后打上就可以了： 12345678910111213141516$ git log --pretty=oneline --abbrev-commit12a631b (HEAD -&gt; master, tag: v1.0, origin/master) merged bug fix 1014c805e2 fix bug 101e1e9c68 merge with no-fff52c633 add mergecf810e4 conflict fixed5dc6824 &amp; simple14096d0 AND simpleb17d20e branch testd46f35e remove test.txtb84166e add test.txt519219b git tracks changese43a48b understand how stage works1094adb append GPLe475afc add distributedeaadf4e wrote a readme file 比方说要对add merge这次提交打标签，它对应的commit id是f52c633，敲入命令： 1$ git tag v0.9 f52c633 再用命令git tag查看标签： 123$ git tagv0.9v1.0 注意，标签不是按时间顺序列出，而是按字母排序的。可以用git show &lt;tagname&gt;查看标签信息： 123456789$ git show v0.9commit f52c63349bc3c1593499807e5c8e972b82c8f286 (tag: v0.9)Author: Yaohua Guo &lt;guo.yaohua@foxmail.com&gt;Date: Fri May 18 21:56:54 2018 +0800 add mergediff --git a/readme.txt b/readme.txt... 可以看到，v0.9确实打在add merge这次提交上。 还可以创建带有说明的标签，用-a指定标签名，-m指定说明文字： 1$ git tag -a v0.1 -m "version 0.1 released" 1094adb 用命令git show &lt;tagname&gt;可以看到说明文字： 123456789101112131415$ git show v0.1tag v0.1Tagger: Yaohua Guo &lt;guo.yaohua@foxmail.com&gt;Date: Fri May 18 22:48:43 2018 +0800version 0.1 releasedcommit 1094adb7b9b3807259d8cb349e7df1d4d6477073 (tag: v0.1)Author: Yaohua Guo &lt;guo.yaohua@foxmail.com&gt;Date: Fri May 18 21:06:15 2018 +0800 append GPLdiff --git a/readme.txt b/readme.txt... 操作标签如果标签打错了，也可以删除： 12$ git tag -d v0.1Deleted tag 'v0.1' (was f15b0dd) 因为创建的标签都只存储在本地，不会自动推送到远程。所以，打错的标签可以在本地安全删除。 如果要推送某个标签到远程，使用命令git push origin &lt;tagname&gt;： 1234$ git push origin v1.0Total 0 (delta 0), reused 0 (delta 0)To git@github.com:guoyaohua/learngit.git * [new tag] v1.0 -&gt; v1.0 或者，一次性推送全部尚未推送到远程的本地标签： 1234$ git push origin --tagsTotal 0 (delta 0), reused 0 (delta 0)To git@github.com:guoyaohua/learngit.git * [new tag] v0.9 -&gt; v0.9 如果标签已经推送到远程，要删除远程标签就麻烦一点，先从本地删除： 12$ git tag -d v0.9Deleted tag 'v0.9' (was f52c633) 然后，从远程删除。删除命令也是push，但是格式如下： 123$ git push origin :refs/tags/v0.9To git@github.com:guoyaohua/learngit.git - [deleted] v0.9 要看看是否真的从远程库删除了标签，可以登陆GitHub查看。 自定义Git忽略特殊文件有些时候，你必须把某些文件放到Git工作目录中，但又不能提交它们，比如保存了数据库密码的配置文件啦，等等，每次git status都会显示Untracked files ...，有强迫症的朋友心里肯定不爽。 好在Git考虑到了大家的感受，这个问题解决起来也很简单，在Git工作区的根目录下创建一个特殊的.gitignore文件，然后把要忽略的文件名填进去，Git就会自动忽略这些文件。 不需要从头写.gitignore文件，GitHub已经为我们准备了各种配置文件，只需要组合一下就可以使用了。所有配置文件可以直接在线浏览：https://github.com/github/gitignore 忽略文件的原则是： 忽略操作系统自动生成的文件，比如缩略图等； 忽略编译生成的中间文件、可执行文件等，也就是如果一个文件是通过另一个文件自动生成的，那自动生成的文件就没必要放进版本库，比如Java编译产生的.class文件； 忽略你自己的带有敏感信息的配置文件，比如存放口令的配置文件。 举个例子： 假设你在Windows下进行Python开发，Windows会自动在有图片的目录下生成隐藏的缩略图文件，如果有自定义目录，目录下就会有Desktop.ini文件，因此你需要忽略Windows自动生成的垃圾文件： 1234# Windows:Thumbs.dbehthumbs.dbDesktop.ini 然后，继续忽略Python编译产生的.pyc、.pyo、dist等文件或目录： 1234567# Python:*.py[cod]*.so*.egg*.egg-infodistbuild 加上你自己定义的文件，最终得到一个完整的.gitignore文件，内容如下： 12345678910111213141516# Windows:Thumbs.dbehthumbs.dbDesktop.ini# Python:*.py[cod]*.so*.egg*.egg-infodistbuild# My configurations:db.inideploy_key_rsa 最后一步就是把.gitignore也提交到Git，就完成了！当然检验.gitignore的标准是git status命令是不是说working directory clean。 使用Windows的朋友注意了，如果你在资源管理器里新建一个.gitignore文件，它会非常弱智地提示你必须输入文件名，但是在文本编辑器里“保存”或者“另存为”就可以把文件保存为.gitignore了。 有些时候，你想添加一个文件到Git，但发现添加不了，原因是这个文件被.gitignore忽略了： 1234$ git add App.classThe following paths are ignored by one of your .gitignore files:App.classUse -f if you really want to add them. 如果你确实想添加该文件，可以用-f强制添加到Git： 1$ git add -f App.class 或者你发现，可能是.gitignore写得有问题，需要找出来到底哪个规则写错了，可以用git check-ignore命令检查： 12$ git check-ignore -v App.class.gitignore:3:*.class App.class Git会告诉我们，.gitignore的第3行规则忽略了该文件，于是我们就可以知道应该修订哪个规则。 配置别名有没有经常敲错命令？比如git status？status这个单词真心不好记。 如果敲git st就表示git status那就简单多了，当然这种偷懒的办法我们是极力赞成的。 我们只需要敲一行命令，告诉Git，以后st就表示status： 1$ git config --global alias.st status 好了，现在敲git st看看效果。 当然还有别的命令可以简写，很多人都用co表示checkout，ci表示commit，br表示branch： 123$ git config --global alias.co checkout$ git config --global alias.ci commit$ git config --global alias.br branch 以后提交就可以简写成： 1$ git ci -m "bala bala bala..." --global参数是全局参数，也就是这些命令在这台电脑的所有Git仓库下都有用。 在撤销修改一节中，我们知道，命令git reset HEAD file可以把暂存区的修改撤销掉（unstage），重新放回工作区。既然是一个unstage操作，就可以配置一个unstage别名： 1$ git config --global alias.unstage 'reset HEAD' 当你敲入命令： 1$ git unstage test.py 实际上Git执行的是： 1$ git reset HEAD test.py 配置一个git last，让其显示最后一次提交信息： 1$ git config --global alias.last 'log -1' 这样，用git last就能显示最近一次的提交： 1234567$ git lastcommit adca45d317e6d8a4b23f9811c3d7b7f0f180bfe2Merge: bd6ae48 291bea8Author: Yaohua Guo &lt;Guo.Yaohua@foxmail.com&gt;Date: Thu Aug 22 22:49:22 2013 +0800 merge &amp; fix hello.py 甚至可以进一步美化把lg配置成： 1$ git config --global alias.lg "log --color --graph --pretty=format:'%Cred%h%Creset -%C(yellow)%d%Creset %s %Cgreen(%cr) %C(bold blue)&lt;%an&gt;%Creset' --abbrev-commit" 来看看git lg的效果： 配置文件配置Git的时候，加上--global是针对当前用户起作用的，如果不加，那只针对当前的仓库起作用。 配置文件放哪了？每个仓库的Git配置文件都放在.git/config文件中： 12345678910111213141516$ type .git/config [core] repositoryformatversion = 0 filemode = true bare = false logallrefupdates = true ignorecase = true precomposeunicode = true[remote "origin"] url = git@github.com:michaelliao/learngit.git fetch = +refs/heads/*:refs/remotes/origin/*[branch "master"] remote = origin merge = refs/heads/master[alias] last = log -1 别名就在[alias]后面，要删除别名，直接把对应的行删掉即可。 而当前用户的Git配置文件放在用户主目录下的一个隐藏文件.gitconfig中： 123456789$ type .gitconfig[alias] co = checkout ci = commit br = branch st = status[user] name = Your Name email = your@email.com 配置别名也可以直接修改这个文件，如果改错了，可以删掉文件重新通过命令配置。 总结 Git记录的是文件的修改状态，而不是文件本身。 初始化一个Git仓库，使用git init命令。 添加文件到Git仓库，分两步： 使用命令git add &lt;file&gt;，注意，可反复多次使用，添加多个文件； 使用命令git commit -m &lt;message&gt;，完成。 每次修改，如果不用git add到暂存区，那就不会加入到commit中。 提交后，可用git diff HEAD -- &lt;file_name&gt;命令可以查看工作区和版本库里面最新版本的区别。 要关联一个远程库，使用命令git remote add origin git@server-name:path/repo-name.git，使用命令git push -u origin master第一次推送master分支的所有内容，此后，每次本地提交后，只要有必要，就可以使用命令git push origin master推送最新修改。 要克隆一个仓库，首先必须知道仓库的地址，然后使用git clone命令克隆。Git支持多种协议，包括https，但通过ssh支持的原生git协议速度最快。 HEAD指向的版本就是当前版本，因此，Git允许我们在版本的历史之间穿梭，使用命令git reset —hard commit_id`。 穿梭前，用git log可以查看提交历史，以便确定要回退到哪个版本。 要重返未来，用git reflog查看命令历史，以便确定要回到未来的哪个版本。 当你改乱了工作区某个文件的内容，想直接丢弃工作区的修改时，用命令git checkout -- file。 当你不但改乱了工作区某个文件的内容，还添加到了暂存区时，想丢弃修改，分两步，第一步用命令git reset HEAD &lt;file&gt;，第二步按上一条操作。 已经提交了不合适的修改到版本库时，想要撤销本次提交，参考版本回退，不过前提是没有推送到远程库。 命令git rm用于删除一个文件。如果一个文件已经被提交到版本库，那么你永远不用担心误删，但是要小心，你只能恢复文件到最新版本，你会丢失最近一次提交后你修改的内容。 Git鼓励大量使用分支： 查看分支：git branch 创建分支：git branch &lt;name&gt; 切换分支：git checkout &lt;name&gt; 创建+切换分支：git checkout -b &lt;name&gt; 合并某分支到当前分支：git merge &lt;name&gt; 删除分支：git branch -d &lt;name&gt; 当Git无法自动合并分支时，就必须首先解决冲突。解决冲突后，再提交，合并完成。解决冲突就是把Git合并失败的文件手动编辑为我们希望的内容，再提交。 用git log --graph命令可以看到分支合并图。 合并分支时，加上--no-ff参数就可以用普通模式合并，合并后的历史有分支，能看出来曾经做过合并，而fast forward合并就看不出来曾经做过合并。 切换分支使用git checkout &lt;master&gt; ，HEAD指向master，工作区也恢复到master的状态。 开发一个新feature，最好新建一个分支。 如果要丢弃一个没有被合并过的分支，可以通过git branch -D &lt;name&gt;强行删除。 查看远程库信息，使用git remote -v。 本地新建的分支如果不推送到远程，对其他人就是不可见的。 从本地推送分支，使用git push origin branch-name，如果推送失败，先用git pull抓取远程的新提交。 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致。 建立本地分支和远程分支的关联，使用git branch --set-upstream branch-name origin/branch-name。 从远程抓取分支，使用git pull，如果有冲突，要先处理冲突。 命令git tag &lt;tagname&gt;用于新建一个标签，默认为HEAD，也可以指定一个commit id。 命令git tag -a &lt;tagname&gt; -m &quot;blablabla...&quot;可以指定标签信息。 命令git tag可以查看所有标签。 忽略某些文件时，需要编写.gitignore。 .gitignore文件本身要放到版本库里，并且可以对.gitignore做版本管理。]]></content>
      <categories>
        <category>Git</category>
      </categories>
      <tags>
        <tag>Git</tag>
        <tag>Git教程</tag>
        <tag>Git常用命令</tag>
        <tag>Git常用操作</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[深度学习工作站攒机指南]]></title>
    <url>%2Fdeeplearning-workstation%2F</url>
    <content type="text"><![CDATA[引言接触深度学习已经快两年了，之前一直使用Google Colab和Kaggle Kernel提供的免费GPU（Tesla K80）训练模型（最近Google将Colab的GPU升级为Tesla T4，计算速度又提升了一个档次），不过由于内地网络的原因，Google和Kaggle连接十分不稳定，经常断线重连，一直是很令人头痛的问题，而且二者均有很多限制，例如Google Colab一个脚本运行的最长时间为12h，Kaggle的为6h，数据集上传也存在问题，需要使用一些Trick才能达成目的，模型的保存、下载等都会耗费很多精力，总之体验不是很好，不过免费的羊毛让大家撸，肯定会有一些限制，也是可以理解的。 对于租用云服务器，之前也尝试过，租用了一家小平台的GPU服务器，也存在一些操作上的困难，不适合程序调试，而且价格也不便宜。 很早之前就想要搭建一个自己的深度学习工作站，不过机器成本的昂贵，一直阻碍着我攒机计划的进行。工欲善其事，必先利其器！最近终于下定决心，置办一个深度学习工作站主机。本文将我在这段时间选择、购置硬件的心得体会，分享给大家。 配置清单 配件 品牌型号 数量 价格 渠道 CPU Intel 酷睿i7 6950X 至尊版 1 3300 散片 主板 华硕 RAMPAGE V EXTREME X99 主板 1 1085 二手 内存 海盗船 复仇者LPX 16GB DDR4 2400 4 1920 全新 HDD 西部数据 WD40EZRZ 蓝盘 4TB 1 550 OEM SSD 西数 WDS100T2X0C 黑盘 1TB 1 1220 全新 显卡 影驰 GeForce RTX 2070 大将 1 3400 全新 机箱 爱国者（aigo）月光宝盒 破晓 1 264 全新 电源 鑫谷 GP1350G 1250W 1 680 全新 散热器 爱国者（aigo）冰塔T240 极光版 1 299 全新 风扇 金河田 光影炫光 12CM 6 60 全新 12778 总计 以上就是我主机的配置清单，目前只买了一张2070，后期会升级加入多卡，下面将详细分析一下各个配件的选购过程。 配件选购指南主板有很多朋友在进行选购主机的时候认为应该先选CPU再选主板，个人认为配件选购的顺序和主机的用途是有关系的，对于搭建深度学习工作站而言，在正式进行硬件选购前，最重要的是需要确认一个问题，到底需要单卡（GPU）主机还是多卡主机，如果只需要搭建单显卡的主机，那么在选购主板的过程中，不需要花费太多精力，大量主板可以满足要求，如果想要搭建双卡、三卡或是四卡主机，则需要在主板上下点功夫，为了日后升级方便，我的目标是使用可支持四显卡的主机，所以在主板选择方面，会很注重PCIE扩展接口数量。 在初期选择主板时，网上各式各样型号的主板会使小萌新（我）很是懵逼，在网上查找了些资料，了解了些主板的知识。为了保证CPU和主板搭配合理，装到一起能正常工作，首先我们需要了解各主板芯片组和CPU接口的具体含义。例如，下图为京东的主板截图。 我们可以看到大多数商品名称后面都会有一个类似（AMD A320/Socket AM4）或（Intel H310/LGA 1151）的注释，其实这些就是影响你CPU和主板能否匹配的最重要参数了，在商品详情页，我们也可以找到这个参数，前面的“Intel H310”或“AMD A320”指的就是主板的芯片组，而后面的“LGA 1151”或“Socket AM4”指的就是主板上CPU插槽的类型了。 芯片组芯片组示主板的核心芯片，选对芯片组，主板和CPU才能兼容。目前主流的主板分为Intel和AMD两个系列，分别对应不同品牌的处理器。而每个系列又按照芯片组类型的不同，分为很多子系列。以Intel系列主板为例，在市面上可以看到华硕、技嘉、七彩虹等近十个品牌的产品，不同品牌的主板在外观和技术上会有一些差别，但他们使用的芯片组都是由Intel提供的。 不过，虽然同属于Intel系列主板，但根据处理器的不同，需要搭配对应芯片组的主板才能成功组建出一台可以使用的主机。比如目前Intel最新的九代酷睿 i9-9900k 处理器需要搭配Z390、Z370或H370芯片组的主板来使用。而AMD的Ryzen 3/5/7系列CPU和APU产品则可以搭配X370、B350或A320芯片组的主板。 那么不同芯片组的主板又有什么区别呢？有的时候，多个芯片组的主板虽然可以支持同一款处理器，但在主板的规格上还是有一定区别的。这些区别包括但不限于原生USB及磁盘接口数量、是否支持CPU超频、是否支持多显卡互联等。这对于不太了解主板的用户来说确实很难选择，简单总结一下： B系列（如B360、B250）属于入门级产品，不具备超频和多卡互联的功能，同时接口及插槽数量也相对要少一些。H系列（如H310）比B系列略微高端一些，可以支持多卡互联，接口及插槽数量有所增长。Z系列（如Z390、Z370）除了具备H系列的特点支持，还能够对CPU进行超频，并且接口和插槽数量也非常丰富。X系列（如X99、X299）可支持Intel至尊系列高端处理器，同时具备Z系列的各项特点。 同时，Intel的100系列和200系列主板可以搭配6代及7代酷睿处理器，300系列主板需要搭配8代酷睿处理器，X299系列主板需要搭配7代至尊系列酷睿处理器。 对于单路CPU的主板，能够同时支持四张显卡卡的神板，毫无疑问就只有X99/X299系列的主板了，当然你也可以考虑intel 服务器C系列多路CPU主板，可以支持两个CPU在一张主板上。我的目标是使用单路CPU，所以也就没有关注C系列主板。 对于X299和X99之间的选择，有的朋友会主张买新不买旧，我个人的建议还是性价比高才是好的，较新的X299板子相比X99主板要贵大几百甚至1k左右，功能上的提升并不是很大，对于我们大多数Deep Learning开发者而言，X99的板子足够了，毕竟要把钱花在刀刃上，GPU才是大手笔。X99板子主要推荐以下三款： Asus/华硕 X99-E WS/USB 3.1 Asus/华硕 RAMPAGE V EXTREME/U3.1 MSI/微星 X99S GAMING 7 对比 型号名称 MSI/微星 X99S GAMING 7 华硕RAMPAGE V EXTREME/U3.1 华硕X99-E WS/USB 3.1 主芯片组 Intel X99 Intel X99 Intel X99 CPU插槽 LGA 2011-v3 LGA 2011-v3 LGA 2011-v3 内存规格 8×DDR4 DIMM 四通道 8×DDR4 DIMM 四通道 8×DDR4 DIMM 四通道 最大内存容量 128GB 128GB 128GB PCI-E标准 PCI-E 3.0 PCI-E 3.0 PCI-E 3.0 PCI-E插槽 4×PCI-E X16 插槽 5×PCI-E X16 插槽1×PCI-E X1 插槽 7×PCI-E X16 插槽 存储接口 10×SATA III1×SATA Express1×M.2（10Gb/s） 1×M.22×SATA Express8×SATA III 1×M.22×SATA Express8×SATA III2×eSATA USB接口 6×USB2.0（2背板+4内置）12×USB3.0（4背板+8内置） 14×USB3.0（4内置+10背板）6×USB2.0（4内置+2背板） 14×USB3.0（4内置+10背板）4×USB2.0（4内置） 主板板型 ATX板型 E-ATX板型 E-ATX板型 外形尺寸 30.5×24.4cm 30.5×27.2cm 30.5×26.7cm 多显卡技术 NVIDIA 3-Way SLI NVIDIA 3-Way SLI NVIDIA 4-Way SLIAMD 4-Way CrossFireX NVIDIA 4-Way SLIAMD 4-Way CrossFire 可以看到这三款主板，均为X99芯片组，CPU插槽均为 LGA 2011-v3 ，而且有8个内存插槽，支持四通道，最高128G的内存容量，内存容量这部分个人很喜欢，对于大型数据集数据预处理的过程，对内存容量和CPU要求都很高，而且足够的内存容量使你不用再为多开窗口卡顿现象而担忧。三者都支持多显卡扩展，华硕R5E和华硕X99 E-WS均支持4显卡交火，微星X99S Gaming 7支持3显卡交火，不过显卡交火，对于深度学习计算没有任何的帮助，对游戏确是有一些提升，我们日常所说的多显卡训练模型，也不是用到交火技术，而是Data Parallel或Model Parallel，所以交火与否我们不需要关注，需要关注的时PCIE ×16扩展插槽的有效个数（有的间距太近，无法全插）。 起初最想购买的是“华硕 X99-E WS”，经典的工作站主板，很多深度学习开发者的首选，支持四路显卡交火，更为优秀的是竟然有7个×16全速PCIE 3.0扩展插槽，但是对于这类主板虽然有如此强大的扩展功能，但在真正插显卡的时候，由于PCIE接口之间的空间限制，你是无法插满插槽的，而且现在显卡都很厚，很可能会造成接口的浪费。这个板子已经停产，不过在天猫的华硕旗舰店仍然有存货，售价“3899元”，还是很贵的。其中有很多功能，对于我们日常使用、训练模型来讲并不是很用得上，会造成没必要的开销。最后我选择了在淘宝购买二手的“华硕 RAMPAGE V EXTREME”，毕竟便宜。如果经费充足的朋友，我仍然建议购买“华硕 X99 E-WS”这个主板。 CPU对于CPU的选取是基于确定主板CPU插槽类型为前提的，例如我们上文中我们选择的X99系列三款主板，CPU插槽类型均为“LGA 2011-v3”，我们就要选与此匹配的CPU，各插槽类型的CPU具体有哪些型号，可以去中关村在线查询，里面还有一些性能测评的文章和排行榜信息，值得推荐。 为了能够为CPU做出明智的选择，我们首先需要了解CPU以及它与深度学习的关系。CPU为深度学习做了什么？当你在GPU上运行深度网络时，CPU几乎不会进行任何计算。它主要的作用是：（1）启动GPU函数调用，（2）执行CPU函数。 CPU对于数据预处理的过程却起重要作用。有两种常见的数据处理策略，它们具有不同的CPU需求。 1234567891011# 一、在训练时进行数据预处理for train_step in range(tot_train_step): load_mini_batch() preprocess_mini_batch() train_on_mini_batch()# 二、在训练前进行数据预处理preprocess_data()for train_step in range(tot_train_step): load_preprocessed_mini_batch() train_on_mini_batch() 对于第一个策略，为避免CPU的性能成为训练模型的速度的瓶颈，具有高主频多内核的CPU可以显著提高性能，加快训练速度。对于第二种策略，由于是预先进行数据预处理，在训练时的速度取决于GPU性能，与CPU无关，理论上CPU的性能不会成为瓶颈。但是我个人的观点还是，在经费允许的情况下，尽管CPU的性能越强大越好，但是也不一定非要追求最新款、最强大的CPU，性价比和个人需求才是最关键的。 当然，此处附加一点说明，如果攒机后不仅需要训练模型，而且偶尔也会玩一些游戏消遣的话，尽量选择高主频的主机，志强系列多核心低主频CPU不适合游戏玩家。 PCIe 通道CPU的PCIe通道对模型训练的影响网上也纵说纷云，首先让我们先了解一下什么是CPU的PCIe通道 PCI-Express(peripheral component interconnect express)是一种高速串行计算机扩展总线标准，它原来的名称为“3GIO”，是由英特尔在2001年提出的，旨在替代旧的PCI，PCI-X和AGP总线标准。PCIe属于高速串行点对点双通道高带宽传输，所连接的设备分配独享通道带宽，不共享总线带宽，主要支持主动电源管理，错误报告，端对端的可靠性传输，热插拔以及服务质量(QOS)等功能。 简而言之，PCIe通道就是主机中各组件进行数据交互的信道，PCIe通道分两种： CPU直连通道，主流消费级只给你16条(8700K)，高端&amp;服务器上才会多给(7980XE)。 DMI3总线PCH芯片分发出来的，是主板的属性。例如Z370主板声称有24条PCIE通道，其实这24条就是PCH通道，要共享DMI3等效直连PCIe ×4的带宽。 就PCH而言，在很多高性能扩展面前没有智联通道强大，所以对于CPU的直连通道数就显得至关重要了。 Tim Dettmers大神在他的博文《A Full Hardware Guide to Deep Learning》中也做出了对PCIe通路的见解，Tim认为在单机少量（小于4）GPU的主机中，PCIe通路对模型训练的影响并不是很大，但对于大于4个GPU或GPU集群PCIe通路的影响就会很显著。在文章中，Tim对比了不同通道数量在模型训练过程中的速度传输速度对比。 CPU and PCI-Express People go crazy about PCIe lanes! However, the thing is that it has almost no effect on deep learning performance. If you have a single GPU, PCIe lanes are only needed to transfer data from your CPU RAM to your GPU RAM quickly. However, an ImageNet batch of 32 images (32x225x225x3) and 32-bit needs 1.1 milliseconds with 16 lanes, 2.3 milliseconds with 8 lanes, and 4.5 milliseconds with 4 lanes. These are theoretic numbers, and in practice you often see PCIe be twice as slow — but this is still lightning fast! PCIe lanes often have a latency in the nanosecond range and thus latency can be ignored. Putting this together we have for an ImageNet mini-batch of 32 images and a ResNet-152 the following timing: Forward and backward pass: 216 milliseconds (ms) 16 PCIe lanes CPU-&gt;GPU transfer: About 2 ms (1.1 ms theoretical) 8 PCIe lanes CPU-&gt;GPU transfer: About 5 ms (2.3 ms) 4 PCIe lanes CPU-&gt;GPU transfer: About 9 ms (4.5 ms) Thus going from 4 to 16 PCIe lanes will give you a performance increase of roughly 3.2%. However, if you use PyTorch’s data loader with pinned memory you gain exactly 0% performance. So do not waste your money on PCIe lanes if you are using a single GPU! When you select CPU PCIe lanes and motherboard PCIe lanes make sure that you select a combination which supports the desired number of GPUs. If you buy a motherboard that supports 2 GPUs, and you want to have 2 GPUs eventually, make sure that you buy a CPU that supports 2 GPUs, but do not necessarily look at PCIe lanes. PCIe Lanes and Multi-GPU Parallelism Are PCIe lanes important if you train networks on multiple GPUs with data parallelism? I have published a paper on this at ICLR2016, and I can tell you if you have 96 GPUs then PCIe lanes are really important. However, if you have 4 or fewer GPUs this does not matter much. If you parallelize across 2-3 GPUs, I would not care at all about PCIe lanes. With 4 GPUs, I would make sure that I can get a support of 8 PCIe lanes per GPU (32 PCIe lanes in total). Since almost nobody runs a system with more than 4 GPUs as a rule of thumb: Do not spend extra money to get more PCIe lanes per GPU — it does not matter! 不过依我个人的看法，还是要选支持PCIe通道数大一点的CPU，毕竟M.2 NVME SSD就会占据四条通道，如果我们CPU只支持16通道，并且有两个GPU的话，每个GPU只能分到×4的速度，这个总感觉不太好。显卡已经花了那么多钱，我们当然希望它能全速跑，不要由于CPU PCIe通路的短板影响整体的性能，得不偿失。所以我更倾向于选择40条通路的CPU。目前主流CPU大多支持16通道、24通道，对于至尊系列CPU会有支持40通道的，对于大部分志强系列服务器CPU大多数支持44通道。 对比对于志强系列，网上所有渠道都基本是拆机CPU，很多是外国服务器淘汰下的CPU，质量方面，由于没有使用过，不妄加评论。志强系列U特点是核心多，单核主频低，如果对于高并发有需求的朋友，可以优先选择志强系列U，搭配双路服务器主板。但对于我个人来讲，对单核主频要求高一些，所以我更倾向于桌面级CPU。 在我选择CPU的过程中，CPU天梯图对我很有帮助，很直观的展现了Intel/AMD所有CPU的性能排行，也推荐给大家。 经过反复的对比，最终锁定了两款CPU“i7-6850K”和“i7-6950X”，首先首先我先解释一下为什么没有选择最新的九代酷睿系列，目前九代酷睿发布没多久，例如“I9-9900K”也是炒的很火热， 虽然最新系列的CPU单核主频都有所提高，但是核心数并没有太大改善，致使CPU整体性能（多核性能）并不是越新越好，从上方的天梯图我们也可以看出，“I7-6950X”排在”I9-9900K”前面，第二个原因就是，CPU是一个没什么损耗的器件，如果没有变态超频使用的话，全新和二手的U新能没什么区别，对于这些已经停产的CPU，网上流通的都是拆机的二手版本，只要选择正式版（不要QS/ES版本），其实都可以的，而且价格便宜，性价比极高，例如我们Intel官网可以看到“i7-6950X”的官方报价为￥11053.74，上万元！ 而在淘宝等渠道购买的正式版I7-6950X散片，只有3400元，价格差距如此之大。对于这种旧款CPU的散片（正式版）性价比还是蛮高的，所以推荐大家购买散片CPU（当然也会有很多朋友担心散片CPU的质量问题，这个确实不能保证每个渠道的U都是好的，看人品吧~）。下面对比一下“i7-6850K”、“I7-6950X”、“i75960X”以及“I7-6900K”这几款CPU。 基本要素 酷睿™ i7-6950X 酷睿™ i7-6850K 酷睿™ i7-5960X 酷睿™ i7-6900K 发行日期 Q2’16 Q2’16 Q3’14 Q2’16 光刻 14 nm 14 nm 22 nm 14 nm 内核 10 6 8 8 线程 20 12 16 16 基本频率 3.00 GHz 3.60 GHz 3.00 GHz 3.20 GHz 睿频频率 3.50 GHz 3.80 GHz 3.50 GHz 3.70 GHz 缓存 25 MB 15 MB 20 MB 20 MB TDP 140 W 140 W 140 W 140 W 最大内存 128 GB 128 GB 64 GB 128 GB 内存类型 DDR4 2400/2133 DDR4 2400/2133 DDR4 1600/1866/2133 DDR4 2400/2133 内存通道 4 4 4 4 PCIe通道 40 40 40 40 可以看到，四者均支持40条直通PCIe通道，当主板支持四路交火时，以“华硕RAMPAGE V EXTREME”为例，四张显卡分别占用（×8，×8，×8，×8）的速度，不会对性能产生太大影响。“I7-5960X”仅支持最高64GB的内存容量，这个对于我们的板子就略显不足了，相比“I7-6850K”和“I7-6900K”，在单核主频方面6850k基本频率为3.60GHz高于6900K的3.20GHz，不过核心数却没有胜出，二者基本属于同一等级的CPU，整体性能6900K略高于6850K，不过从性价比方面来看（散片），6850K的性价比还是略高的，散片售价仅2500元左右。“I7-6950X”是这几款U中性能最强劲的，虽然单核主频只有3.00GHz但是10核心20线程使其整体性能遥遥领先，可以从上文中的CPU天梯图中看到，尽管这个U是16年推出的，不过在当日的排行版也能排列到十几名，表现还是很出色的，唯一的不足就是价格略显昂贵，散片价格为3400元，之前一直想买2500元的6850K，后来一咬牙买了6950X。 总之，在选购CPU时，建议需要以下顺序： 筛选与主板插槽类型匹配的CPU。 查看CPU天梯图，了解CPU的整体性能排序。 在intel官网上将预选出的商品进行详细参数对比。 淘宝、京东对比散片价格，考虑入手性价比高的U。 个人建议：由于CPU这个东东如果正常使用基本没什么损耗，在选购散片时，会发现各个商家价格会有所差别，其实不一定要买最贵的，价格的不同可能是由于商家进货渠道引起的，只要认准“正式版”即可，避开“QS/ES”版本。 内存内存是相对好选的组件，就没有必要多说了，以我个人来看，当然容量越大越好，毕竟现在内存价格低谷，抓紧买！ 建议“海盗船复仇者”系列，我在选购内存的时候，基本把淘宝翻了个遍，看了很多厂家，价格都很贵，而且有很多都是小厂。对于海盗船复仇者系列内存，很多DIY玩家的首选，也不是没有原因的，现在一条16G的台式内存，只卖不到500元，很是便宜！而且口碑一直不错。 频率目前市场上的内存大多在频率上做了很多营销手段，我们可以看到（2133 MHz、2400MHz、3000MHz、3200MHz以至于更高）的内存频率，同容量不同频率的内存条价格也相差很多，贵几百元的都有。但是我们是否有必要追求高频率的内存呢？ 经过阅读网上一些大牛的文章，大多数人的观点都是，RAM频率对性能方面没有明显的提升，尤其是在做深度学习方面。其实频率只是各个厂商的一种营销手段，RAM公司会引诱大家购买“更快”的内存，其性价比并不高。有追求频率的钱，还不如多加一条内存。此处可参阅“Does RAM speed REALLY matter?” 所以我入手了“海盗船 复仇者”系列单条16GB最便宜的内存2400 MHz，一共上了4条，总共64GB。主板有8个内存插槽，先插四条构成四通道，剩余的留作扩展，不过此处一定注意， 在安装内存条的时候需要阅读主板说明书，基本每个主板都会给出推荐的插法，看好个插槽所在的通道，一定不要插错了。 SSD在初期选购固态硬盘的朋友可能会经常看到SATA3固态硬盘与M.2固态硬盘，可能有朋友会有疑问：M.2是什么意思？和SATA3固态硬盘有什么区别？下面我们就简单科普M.2接口。 M.2接口M.2是一种固态硬盘新接口，是Intel推出的一种替代MSATA新的接口规范，也就是我们以前经常提到的NGFF，英文全称为：Next Generation Form Factor。 M.2接口固态硬盘主要优势在于体积相比传统的SATA3.0、MSATA更小，并且读取速度更快，对于一些移动设备兼容性更好。 M.2和SATA3固态硬盘的区别 目前固态硬盘（SSD）常用的接口主要有3种： SATA3 - 外形尺寸是2.5寸硬盘的标准尺寸，与2.5寸机械硬盘可以互换。 mSATA - 接口尺寸与笔记本内置无线网卡相同，不过mSATA SSD的外形尺寸比无线网卡更大。 M.2 - 初期称为NGFF接口，是近两年新出的接口，为专门为超小尺寸而设计的，使用PCI-E通道，体积更小，速度更快。 值得一提的是M.2接口固态硬盘又分为：SATA和PCI-E两种，虽说长得一模一样，但性能却是差之千里。此外，有些主板的M.2接口不一定对其支持，所以在买M.2固态硬盘的时候，还需要了解下接口兼容性，在购买主板和SSD时关注一下接口信息即可。 M.2有两种接口定义：Socket 2和Socket 3。Socket 2支持SATA或PCIe ×2通道的SSD，Socket 3专为高性能存储设计，支持PCIe ×4通道。在购买SSD时一定要确认是否走PCIe ×4通道，这样才性能最佳。 容量在容量方面，大多数用户512G就够用了，毕竟SSD不是作为数据存储盘使用，而是作为系统盘安装程序使用，但由于目前SSD价格下滑，决定还是入手一个1T的，这样更充足。 经过仔细挑选，最终筛选出两款SSD，“Samsung/三星 MZ-V7S1T0BW 970 EVO Plus 1TB”和“WD/西部数据 WDS100T2X0C 1TB”，比较推荐三星这款，三星的固态一直是业界最好的，速度最快的，不过西数的SSD也在第一梯队，二者价格相差300元左右，“三星970 EVO Plus”速度略高于“西数WDS100T2X0C”，官方宣传三星这款读取速度3500Mb/s、写入速度3300Mb/s，西数这款读取速度3400Mb/s、写入速度2800Mb/s，如果经费允许的朋友推荐三星这款，不过我选择了较为便宜的西数SSD。其他品牌的没用过，不能妄加评论，但是三星和西数的固态硬盘一定是第一梯队了。 机械硬盘机械硬盘容量视个人情况而定，对于计算机视觉工程师，如果需要存放大体积数据集，就买大一点的，毕竟HDD不贵（但一定记住避开希捷、避开希捷、避开希捷！之前笔记本加装过希捷硬盘，太渣了）。我最后入手了“西部数据 WD40EZRZ 4TB 蓝盘”，买的OEM版本，比官方价格低100多，用着还不错。 显卡显卡的选择，Tim Dettmers的《Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning》一文已经分析的非常透彻，我也是参考了他的文章购置了RTX 2070显卡，各位朋友可以深入阅读该文章，此处只做简要阐述。 性能对比 GPU和TPU的标准化性能数据。越高越好。RTX卡使用16位计算。使用PyTorch 1.0.1和CUDA 10完成基准测试。 性价比分析 卷积网络（CNN），循环网络（RNN）和Transformer的性能/价格。越高越好。RTX 2060的成本效率是Tesla V100的5倍以上。使用PyTorch 1.0.1和CUDA 10完成基准测试。 注：以上图转载自《Which GPU(s) to Get for Deep Learning: My Experience and Advice for Using GPUs in Deep Learning》 经过Tim的分析，更倾向于购买RTX 20系列显卡，因为其独有的“Tensor core”可以支持加速FP16的运算，减少显存的支出，也能减少计算复杂度， 整体建议（转载） 最佳GPU：RTX 2070避开GPU：任何Tesla; 任何Quadro卡; Titan RTX，Titan V，Titan XP实惠但价格昂贵：RTX 2070经济实惠且价格便宜：RTX 2060，GTX 1060（6GB）。我有点钱：GTX 1060（6GB）我几乎没有钱：GTX 1050 Ti（4GB）。或者：CPU（原型设计）+ AWS / TPU（训练）; 或者Colab。我做Kaggle：RTX 2070计算机视觉或机器翻译研究员：GTX 2080 Ti，如果您训练非常大的网络获得RTX Titans。NLP研究员：RTX 2080 Ti使用16位。我开始深入学习并且我认真对待它：从RTX 2070开始。在6-9个月之后购买更多RTX 2070并且您仍然希望投入更多时间进行深度学习。根据您接下来选择的区域（启动，Kaggle，研究，应用深度学习），销售GPU并在大约两年后购买更合适的东西。我想尝试深度学习，但我并不认真：GTX 1050 Ti（4或2GB）。这通常适合您的标准桌面，不需要新的PSU。如果它适合，不要购买新电脑！ 选购看了Tim给出的建议，我最终选择了RTX 2070显卡，虽然只有8G显存，不过使用16位运算，主流网络也都能跑起来。对于品牌的选购我只建议不要买灯效、不要买超频，你要相信一点，同一个型号的显卡（例如RTX 2070），任何厂家任何款式的商品在性能上均没有什么性能差距，价格差距主要体现在所谓的超频、灯光效果上，我个人的建议是完全没有必要把钱花费在这上面，关注一下散热，一线厂商的用料都不会太差。 我选购了“影驰 RTX2070 大将”，3400元。对于经费充裕的朋友，RTX 2080 Ti也是一个非常好的选择，我之后扩展可能会入手2080 Ti 电源通常，我们需要一个足以容纳所有未来GPU的电源。电源的稳定性也对组件的寿命有很大影响，因此购买一个优质的电源是很有意义的。 因为一个主机最终要的功耗组件是GPU和CPU，我们可以通过将CPU和GPU的功耗累加，并且附加其他组件大约额外10％W来计算所需的功率。网上也有很多计算功率的网站，但大多组件不是很全面，没有什么参考价值， 我的建议就是尽量上大功率电源，留作之后升级显卡使用。还要注意，模组电源支持的PCIE口数量，例如上面我所购买的RTX2070影驰大将，电源接口为8+6pin的，占用默许电源两个8pin的PCIe供电口，如果电源给的供电口只有6个8pin供电口，那么我们只能给3张显卡供电。这个问题需要注意一下。 对于电源，我有两款产品推荐：“鑫谷GP1350G 额定1250W 全模组”和“长城巨龙服务器电源 1250W 全模组”，二者价格差不多，我买的鑫谷这款。 散热器CPU散热部分，之前一直想上一个风冷节省支出，但是由于I7-6950X已经140W了，风冷根本压不住，所以上了240双冷排的水冷，在选购散热器时注意与商家咨询该散热器是否能压住该功率CPU。我入手的“爱国者（aigo）冰塔T240 极光版”，散热效果很好。 机箱机箱看个人喜好吧，尽量大一点，散热能好些。我入手的“爱国者（aigo）月光宝盒 破晓”。 风扇风扇这个东西还挺贵的，普通的大约20多一个，真是不明白贵在哪里，购买风扇踩了个坑，以为各种风扇都一样，买便宜的就好，入手了京东最便宜的风扇10元一个，看标注风力之类的都比爱国者极光好，但是实测风力没有爱国者极光风扇强大，不过极光风扇噪音有点大。 组装装机部分随便上张图意思一下吧~总之是忙了一整天才弄好。 性能测试]]></content>
      <categories>
        <category>Deep Learning</category>
      </categories>
      <tags>
        <tag>DeepLearning</tag>
      </tags>
  </entry>
</search>
